\documentclass[12pt]{article}
\usepackage{german}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsmath,mathtools} %hinteres dient der umrandung von Formeln

\usepackage{graphicx}
\usepackage{geometry}
\geometry{a4paper,left=40mm,right=20mm, top=40mm, bottom=20mm}
\pagestyle{plain}
\parindent0cm
\topmargin -1cm
\textheight 25cm
\textwidth 16.0cm
\oddsidemargin -0.0cm

\usepackage{epsf}

\begin{document}
\section*{Vorwort}
Die Bedeutung der Computersimulation für die physikalische Forschung steigt. Man kann von einem 3. methodischen Standbein der Physik sprechen: 
\begin{figure}[ht]
	\centering
  \includegraphics[width=0.45\textwidth]{Null_DrittesStandbein.jpg}
	%\caption{}
	\label{Null_DrittesStandbein}
\end{figure}
\begin{itemize}
\item Vorteile:

\begin{itemize}
\item löst Probleme für die es keine analytische Lösung gibt
\item schafft Raum für Modelle, zu denen es keine analytischen Verfahren gibt
\item kann physikalische Größen berechnen, die nicht messbar sind 
\item immer leistungsfähiger
\item immer billiger
\end{itemize}


\item Vorgehensweise:

\begin{itemize}
\item Modellierung: realistisch $\longleftrightarrow$ einfach
\item Programmierung / Test
\item Erzeugen von Daten / Optimierung
\item Analyse / Interpretation
\end{itemize}
\end{itemize}

\subsection{Hardware}
Diese Vorlesung beschäftigt sich nicht mit Hardware an sich, aber wir müssen abschätzen, ob ein Problem nummerisch lösbar ist! 

\textbf{Beispiel:} Wie weit kann ein Rechner zählen?
\begin{align*}
S(N)= \sum_{i=1}^N x_i \; \mbox{  beispielsweise mit: } x_i = 1
\end{align*}


\begin{enumerate}
\item \textbf{Rechenleistung:} 
typisch 10 GFLOP/S ('\textbf{G}igia \textbf{F}loating \textbf{P}oint \textbf{O}perations \textbf{P}er \textbf{S}econd', Gleitkommazahlen Operationen) \\
\underline{Zum Beispiel Additionen:} \\ %TODO format passt?
Bei $N=10^{10} $ pro Sekunde \\
$\Rightarrow$  Für die Zahl der Atome  ($N = 10^{23}$) dann $10^{13} s \approx 3 \cdot 10^5 a$

\item \textbf{Speicherbedarf:} \\
'Trivialmodell' eines Rechners: 

\begin{figure}[ht]
	\centering
  \includegraphics[width=0.8\textwidth]{Null_Rechner.png}
	%\caption{}
	\label{Null_DrittesStandbein}
\end{figure}

Informationseinheit: 
\begin{align*}
 1 Byte \;   \widehat{=} & \; 1 \mbox{ Zeichen (z.B. einer Tastatur)} \\
 1 kByte \; \widehat{=} & \;  1 \mbox{ Seite Text (z.B. 50 Zeilen, 20 Seiten}) 
\end{align*}
Typisch beim Programmieren ist die Gleitkommazahl (\textit{'double'} in C):
\begin{align*}
double \; \widehat{=} \; 8
\end{align*}
\begin{align*}
\Rightarrow 10^{10} \mbox{ Zahlen} \; \widehat{ \approx } \; 10^{11} Byte \; \approx \; 100 \; GB
\end{align*}
Typischer \textbf{Hauptspeicher} (bei uns $16 \; GB$) wäre im Bruchteil einer Sekunde voll. Die Zahlen, die der Rechner in einer Sekunde verarbeitet \textit{passen nicht} in den Hauptspeicher, falls Zwischenergebnisse gespeichert werden (ist aber auch nicht nötig). Eine typische \textbf{Festplatte} fasst $500 \; GB$. Hier passen die Daten drauf. 

\item \textbf{Präzision:} \\

\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
 & \textbf{Integer} (ohne Komma) & \textbf{Double} (mit Komma) \\ \hline
 Wertebereich (min/max) & $\pm 2147 \; 483 \; 647 \approx 2 \cdot 10^9$ & $\pm 2 \cdot 10^{308} $ \\
 Genauigkeit & 1 & $2 \cdot 10^{-16} \widehat{=} 17$ Stellen \\ \hline
\end{tabular} 
\end{center}
Vorsicht - der Computer nähert/rendert! \\
\begin{itemize}
\item Der Wertebereich vom Integer wird nach $0,2 s$ verlassen
\item Der Wertebereich vom Double wird nach $2 \cdot 10^6 s \; \widehat{=} \; 23 d$ verlassen.
\end{itemize}
\textbf{Beispiel:} (Addieren) Wo sind die Grenzen?
\begin{align*}
\sum_{i=1}^N 1
\end{align*}
\begin{enumerate}

\item Wenn alle Partialsummen in ein Feld aufgeschrieben werden, läuft der Hauptspeicher nach spätestens $16/100s$ über.

\item nach $0,2s$ reicht Integer Wertebereich nicht mehr %TODO missing word, 

\item Nach $ 2 \cdot 10^6 s \; \widehat{=} \; 23 d$ reicht Double Präzision nicht \\
(Beispielsweise 17 Stellen: $ 1\; 2 \; 3\; 4\; 5\; 6\; 7\; 8\; 9\; 0 \;1\; 2\; 3\; 4 \; 5 \; 6 +1$ obwohl $10^{18} < 2 \cdot 10^{308}$) \\
 %TODO missing words 

\textbf{Beispiel:} (Addition von Double) \\
5 signifikante Stellen - Berechnung von $10^9 + 10^4$:
\begin{align*}
& \mbox{ \textit{Exakt:} }  \;  && = 1 \cdot 10^9 + 0,00001 \cdot 10^9 = 1,00001 \cdot 10^9 \\
& \mbox{ \textit{Rechner:} }  &&= 1,0000 \cdot 10^9 + 0,0000... \cdot 10^9 = 1 \cdot 10^9
\end{align*}

\underline{Bemerkung:} $\Rightarrow $ 1. Kommutativgesetz der Addition gilt nicht nummerisch! 

Denn:
\begin{align*}
1 \cdot 10^9 + \underbrace{ 0,000001 \cdot 10^9 + ... }_{10^{5}} = 1 \cdot 10^9
\end{align*}
während
\begin{align*}
\underbrace{0,00001 \cdot 10^9 + ... }_{10^5} + 1 \cdot 10^9 = 1,0001 \cdot 10^9
\end{align*}
$\Rightarrow$ Regel für den Programmierer: Bei langen Zahlenreihen erst die kleinen addieren! 
\end{enumerate}
\end{enumerate}

\section{Stochastische Physik}

\subsection{Mikrozustände, Phasenraum und Entropie}
Die \textbf{Gesamtheit (Ensemble)} ist eine Menge von Mikrozuständen, die einen Makrozustand repräsentieren (Zeit/Systemabhängig). Der \textbf{Mikrozustand} eines Systems wird durch generalisierte Koordinaten $q_i$ und Impulse $p_i$ mit $i=\{ 1,...,3N\} $ beschrieben. \\
$\to 6N$-dimensionaler Phasenraum. Ein Punkt im Phasenraum ist ein Mikrozustand, Dynamik führt auf eine Bahn (\textit{Phasenraumtrajektorie)}

%
%\begin{figure}[ht]
%	\centering
%  \includegraphics[width=0.8\textwidth]{1}
%	%\caption{Phasenraumtrajektorie des $1d$ harmonischen Oszillators. $q(t) \propto cos( \omega t)$ und $p(t) \propto sin(\\omega t)}
%	\label{Phasenraum}
%\end{figure}

Die \textbf{Hamiltonfunktion} $H(q,p)$ entspricht der Gesamtenergie des Systems. \textbf{Energieerhaltung:} Im abgeschlossenen System ist die Gesamtenergie konstant mit $U = H (q (t), p(t)) = const.$ (\textit{innere Energie}). \\
Die stochastische Beschreibung ist zurückzuführen auf die \textbf{Wahrscheinlichkeitsverteilung} $\rho (p,q)$\footnote{ besser: eine Wahrscheinlichkeitsdichte} mit der Wahrscheinlichkeit $\rho (q,p) dq^{3N} dp	^{3N}= \prod_{i=1}^{N} \; dq_i dp_i =: d\Gamma $  die Mikroszustände im Phasenraumelement $dq^{3N} dp^{3N} $ des Systems zu finden. Die Normierung verlangt: 
\begin{align}
1 = \int \rho (q,p) \; d\Gamma \mbox{ (mit Phasenraumvolumen } d\Gamma)
\end{align}

Das Phasenraumintegral wird also etwas anders definiert:
\begin{enumerate}
\item Faktor $\frac{1}{h^{3N}}$ wird eingeführt, damit $\rho $ dimensionslos ist. Mit dem \textsc{Planck}schen Wirkungsquantum $h$ und macht Sinn wegen $\Delta q \Delta p \approx h$ (kleinstmögliches Phasenraumelement)
\item Bislang gehen wir davon aus, dass die Teilchen ununterscheidbar sind (durchnummerierbar). Bei unterscheidbaren Teilchen wäre das gleiche Zustände! Deswegen muss zusätzlich durch die Zahl der Vertauschungen ($N! $) dividiert werden! (\textsc{Gibbson}scher Korrelationsfaktor)
\begin{align}
1 = \frac{1}{h^{3N} N!} \int \rho (q,p)\;  d\Gamma
\end{align}
Ohne diese Korrektur kommt man zum \textsc{Gibb}schen Paradoxon. Der \textbf{thermische Mittelwert} $A(q,p)$ einer beliebigen Größe ist damit
\begin{align}
\Aboxed{ \langle A \rangle = \frac{1}{h^{3N} N!} \int \rho (q,p)\; A(q,p) \;  d\Gamma }
\end{align}
Dieser ist Nummerisch nicht lösbar, da für 10 Teilchen bereits $10^{60}$ Rechenoperationen nötig wären! 
\end{enumerate}
Die \textbf{Entropie} definieren wir analog zur Informationstheorie über 
\begin{align}
S = - \langle k_B ln(\rho ) \rangle  = -\frac{k_B}{h^{3N} N!} \int \rho \; ln(\rho ) d\Gamma 
\end{align}
Bedeutung der Entropie: 
\begin{itemize}
\item Informationsgehalt und Ungewissheit (\textit{Informationstheorie});
\item  thermisches Mittel, 'Unordnung' (\textit{Physik})
\end{itemize}  

\textbf{Beispiel:} (gezinkter Münzwurf)\\
Es sei $p(Kopf) = p \neq p(Zahl)= q$ und $1=p+q$. %TODO zweimal gleiches P? oder Pp?
\begin{align*}
\Rightarrow S =& - k_B \underbrace{\sum_{_{Kopf, Zahl}} \rho \; ln(\rho ) }_\text{Integral wird Summe} = -k_B  (p \; ln(p) + q \; ln(q)) \\
=&  -k_B ( p \; ln(p) + (1-p) \; ln(1-p))
\end{align*}
Wir werfen N-mal:

\begin{align*}
\begin{rcases}
p= 0: Z\; Z\; Z\; Z\; Z\; Z\; Z\; ... \\ 
p= 1: K\; K\; K\; K\; K\;  ...  
\end{rcases}
\text{ 'Unordnung', 'Informationsgehalt' S = 0} \\
\end{align*}
\begin{align*}
\begin{rcases}
p= 0.5: K\; K\; Z\; K\;  Z\; ...
\end{rcases}
\text{S maximal}
\end{align*}
Im Phasenraum: $S=0$ bestehen überall $\rho = 0$ außer an einem Punkt $\rho(p_0, q_0) = 1$ $\Rightarrow$ nur \textit{ein} Mikrozustand zu vorgegebenen Makrozustand. Hohe Entropie heißt, viele, gleich wahrscheinliche Mikrozustände. 

\subsection{Mikrokanonische Gesamtheit}
\textbf{Ziel allgemein:} Bestimmung von $\rho$ für definierte physikalische Systeme \\
\underline{Hier (mikrokanonisch):} Isoliertes (abgeschlossenes) System mit festem $U,V,N$ (innere Energie, Vauum, Teilchenzahl). \footnote{Zur Erinnerung: \\ Die \textit{kanonische} Gesamtheit: festes N, Energiefluktuation\\ Das\textit{ großkanonische} Ensemble: Energie- und Teilchenfluktuation}

Wir postulieren, dass dann $\rho$ für alle erlaubten Zustände gleich ist, also
\begin{align}
\rho (q,p) = 
\begin{cases}
\rho  & \mbox{ für } H(p,q) = U \\
0 & \mbox{ sonst }
\end{cases}
\end{align}
Da $\rho $ Volumenintegral, ist es zweckmäßig, Volumen zwischen zwei Hyperfllächen zu betrachten: 
\begin{align}
\beta = \{ (p,q): U \leq H (p,q) \leq U +\Delta \}
\end{align}
später dann $\Delta \to 0$. 
\begin{align}
\int_\beta \underbrace{ \; d \Gamma }_{ inkl.  \; (h^{3N} N!)^{-1}  } = 1 \; \Rightarrow \rho = \frac{1}{Z}
\end{align} 

mit der \textbf{Zustandssumme der mikrokanonischen Gesamtheit}
\begin{align}
Z_\Delta = \int_\beta d\Gamma = Z_\Delta (U,V,N) 
\end{align} \footnote{U steht in $\beta$, V steckt in $d\Gamma$, N steckt in Dimension des Phasenraums. Z ist eine thermodynamische Größe. Im mikrokanonischen Ensemble führt Z auf S.}
\begin{align}
\Rightarrow S =& - \langle k_B \rho \; ln(\rho ) \rangle = - k_B \underbrace{\int_\beta d\Gamma \rho \; }_{=1} \underbrace{ln(\rho)}_\text{unabhängig von p,q} \\
=& k_B ln (Z_\Delta(U,V,N)) \\
=& - k_B \; ln(\rho )
\end{align}

Zusammenhang zur Thermodynamik: S(U,V,N) ist ein thermodynamisches Potential mit natürlichen Variablen U,V,N
\begin{align}
\begin{rcases}
& \frac{\partial S}{\partial U} \biggr\vert_{V,N} = \frac{1}{T} = k \frac{1}{Z} \left( \frac{\partial Z}{\partial U}\right)_{V,N} \\
& \frac{\partial S}{\partial V} \biggr\vert_{U,N} =  -\frac{p}{T} \\
& \frac{\partial S}{\partial N} = \frac{\mu }{T}
\end{rcases}
 dS = \frac{\partial S}{\partial U} dU +\frac{\partial S}{\partial V}dV+  \frac{\partial S}{\partial N} dN
\end{align}

Abschließende Bemerkungen: 
\begin{itemize}
\item Zustandssumme der mikrokanonischen Gesamtheit kann als 'Zahl der Zustände' zu vorgegebener Energie U interpretiert werden: 
\begin{align}
Z = \frac{1}{h^{3N} N!} \int_{H(p,q)=U} dp \; dq
\end{align}
also Entropie $\propto ln(\mbox{Zahl der Zustände})$
\item Rechnungen in der mikrokanonischen Gesamtheit sind recht kompliziert wegen der Randbedingung $H=U$. Besser sind andere Gesamtheiten. 
\item Statistik erfordert den Limes großer Teilchenzahlen
\end{itemize}
\textbf{Wichtig: löst man nummerisch Bewegungsgleichnungen unter Energieerhaltung, summiert man im mikrokanonischen Ensemble}
\subsection{Kanonisches Ensemble}
Wir betrachten jetzt ein System in Kontakt mit einem Wärmebad der Temperatur $T$: \\
\begin{itemize}
\item Die Gesamtenergie $E = E_w + E_k$. \\
\item Das Wärmebad soll sehr groß sein, sodass $\frac{E_K}{E} << 1$
\item Die Energie in K ist nicht mehr Konstanz, nur im Gesamtsystem. Das Gesamtsystem ist mikrokanonisch, alle Gesamtzustände sind gleich wahrscheinlich. Gesucht ist die Wahrscheinlichkeitsdichte $\rho (p,q) = \rho_i $ der Mikrozustände in K. 
\item $\rho_i$ gehört zum Mikrozustand mit Energie $E_i$ (siehe links) %TODO what? LEft? 
Zahl der Makrozustände des Gesamtsystems ist gleich der Zahl der Mikrozustände des Wärmebads mit $E_w = E - E_i$ (bzw. gleich der mikrokanonischen Zustandssumme) 
\begin{align}
Z_w (E- E_i): \rho_i \propto Z_w (E-E_i)
\end{align}
Betrachte Entropie
\begin{align}
S_w &= k_B ln (Z_w (E-E_i)) \\
&= k_B ln (Z_w (E)) - \underbrace{ k_B \frac{\partial ln (Z_w (E-E_i))}{\partial E_i}}_{ \frac{1}{T}} E_i \; ( E >> E_i) \\
& \Rightarrow Z_w (E-E_i) = Z_w (E) e^{-\frac{E_i}{k_B T}} \\
&\Rightarrow \rho_i \propto e^{- \frac{E_i}{k_B T} } \; \mbox{  ('\textsc{Boltzmann}-Faktor')} \\
&\Rightarrow \rho (p,q) \propto e^{- \beta \; H(p,q)} \; \mbox{ mit: } \beta = \frac{1}{k_B T}
\end{align}
Die Normierung von $\rho$ verlangt: 
\begin{align}
\rho (p,q) = \frac{e^{- \beta \; H(p,q)}}{Z}
\end{align}
mit
\begin{align}
Z = \int d \Gamma e^{- \beta \; H(p,q)}
\end{align}
der \textbf{Zustandssumme der kanonischen Gesamtheit.} Integral erstreckt sich über den gesamten Phasenraum. \textbf{Thermodynamik:}
\begin{align}
S &= -k_B \int d\Gamma \rho \; ln(\rho ) = -k_B \int d\Gamma \frac{e^{- \beta \; H}}{Z} (- \beta H - ln(Z)) \\
&= \frac{1}{T} \underbrace{ \int d\Gamma \; H \; \rho}_U + k_B ln(Z) \int d\Gamma \; \rho \\
&=\frac{U}{T} + k_B ln(Z)
\end{align}
\begin{align}
&\Rightarrow TS = U + k_B T ln(Z) \; (F= U - TS) \\
& \Rightarrow \mbox{ Freie Energie: } F = - k_B T ln(Z) = F(T,V,N)
\end{align}
\textbf{Beispiel:} (Ideales Gas)
\begin{align*}
H = \sum_{i=1}^N \frac{p_i ^2}{2m} \; ,  \; \vec{p_i} = (p_x, p_y, p_z)
\end{align*}
Kanonische Zustandssumme
\begin{align*}
Z(T,V,N) &= \frac{1}{h^{3N} N!} \int d\Gamma e^{- \beta \sum \frac{p_i ^2}{2m} } = \frac{V^N}{h^{3N} N!} \int dp_1 ^3 ...dp_N ^3 \;  e^{ - \beta \frac{p_1 ^2}{2m}} ... e^{- \beta \frac{p_N ^2}{2m}}  \\
&=  \frac{V^N}{h^{3N} N!} \; \prod_{i=1}^N \underbrace{\int dp_i ^3 \; e^{- \beta \frac{p_i ^2}{2m}} }_{ \sqrt{\frac{2m\pi}{\beta}}} \tag{$i = 1...N$ Teilchen} \\
&= \frac{V^N}{h^{3N} N!} \; \prod_{i=1}^{3N}  \int dp_i  \; e^{- \beta \frac{p_i ^2}{2m}} \tag{$i = 1...3N,$  N Teilchen mit $(x,y,z)$} \\
&= \frac{V^N}{h^{3N} N!} \sqrt[\frac{3N}{2}]{\frac{2m\pi}{\beta}}
\end{align*} %TODO alle p_i Vektoren?
\end{itemize}
Für die freie Energie verwenden wir die \textsc{Stirling}-Formel für große N:
\begin{align}
ln(N!) \approx N \; ln(N) - N
\end{align}
\begin{align}
\Rightarrow F = -k_B T \; ln(Z) = -k_B T N \left[ ln \left(\frac{V}{N}\right) + \frac{3}{2} ln \left(\frac{2 m \pi }{h^3} k_BT \right) +1 \right]
\end{align}

\begin{align}
\Rightarrow p = - \frac{\partial F}{\partial V} \biggr\vert_{T,N} = \frac{N k_B T}{V}
\end{align}

\begin{align}
 \Rightarrow \; \Aboxed{ p V = N k_B T}
\end{align}
Es folgt also eine der Gasgleichungen. Die andere folgt aus
\begin{align}
U = F + TS = F + T \frac{\partial F}{\partial T} \biggr\vert_{V,N} = \frac{3}{2} N k_B T
\end{align}
\textbf{Diskussion:} \\
\begin{itemize}
\item gilt auch für großkanonische Gesamtheiten
\item Berechnung zu verschiedenen Gesamtheiten führen zu gleichen thermodynamischen Potentialen (also 'gleichen Ergebnissen'). $\to$ Nach \textsc{Legendre}-Transformation, Zustandssummen verschieden. \\
Rechnungen jedoch verschieden, Numerik auch! (z.B. Resultate) %TODO stimmt resultate?
\item Phasenraumintegrale werden meist nicht numerisch berechnet %TODO ?? Stimmt der satz?
(\textit{Ausblick:} MD, MC) 
\end{itemize}
\subsection{Weitere Resultate der statistischen Mechanik}
\begin{itemize}
\item \textbf{Extremalprinzip:} im Gleichgewicht werden thermodynamische Potentiale extremal\footnote{genau genomme wir $\rho$ so gewählt}, z. B. 
\begin{align}
& S(U,V,N) \, && \mbox{ maximal} \\
& F(T,V,N), \; U(S,V,N) \; && \mbox{ minimal}
\end{align}
\item \textbf{Schwankungen:} Die Wärmekapazität
\begin{align}
c_v = T \frac{\partial ^2 U}{\partial S ^2} \biggr\vert_{V,N} = \frac{\partial U}{\partial T}  \biggr\vert_{V,N} &= \frac{1}{k_B T^2} \langle (H - \langle H \rangle)^2\rangle \\
&= \frac{1}{k_B T^2} \left( \langle H^2 \rangle - \langle H \rangle ^2 \right) \mbox{ (numerisch geschickt)}
\end{align}
\item \textbf{Gleichverteilungssatz:} jede quadratische Form in H gibt einen Beitrag $\frac{k_B T}{2}$ in U. 
\textbf{Beispiel:} (ideales Gas) 
\begin{align*}
H = \sum_{i=1}^N \frac{p_i ^2}{2m}
\end{align*}
mit $p^2 = p_x^2 + p_y^2 + p_z^2 \Rightarrow U= \frac{3}{2} N k_B T$ (gut für Tests der Numerik) 
\end{itemize}

\section{Monte Carlo Verfahren}
\subsection{Zufallszahlen}
Statistik wichtig für Physik und andere Disziplinen. Physikalische Probleme sind oft ähnlich. Statt beispielsweise$10^{23}$ Atomen werden nur wenige betrachtet, dann 'hochgerechnet'.  \\
$\Rightarrow$ Zahl der Freiheitsgrade sinnvoll beschränken. Häufig wichtig für 'Stichprobe': zufällige Auswahl  $\to$ also Zufallszahl.\\
Im einfachsten Fall: \textit{diskrete Zufallszahlen} $x_1 ... x_N$\\
\textbf{Beispiel:} (Würfel)  \\
\begin{align*}
x_1 = 1 , \; ... \; ,  x_6 = 1 , \; (N=6)
\end{align*}
Die \textbf{Wahrscheinlichkeitsdichte} ist definiert über die
\begin{align}
p_i = \frac{\mbox{Häufigkeit für Auftreten von } x_i}{\mbox{Gesamtzahl der Messungen}} = \frac{m_i}{M} \biggr\vert_{M \to \infty}
\end{align}
\begin{align*}
\sum_{i=1}^N p_i = 1
\end{align*}
\textbf{Beispiel:} (zurück zum Würfel)
\begin{align*}
p_i = \frac{1}{6}, \; i=\{1,...,6\}
\end{align*}
Der \textbf{Mittelwert} ist definiert über
\begin{align}
\overline{x} = \sum_{i=1}^N p_i \; x_i \; = \sum_{i=1}^N \frac{M_i x_i}{M} \biggr\vert_{M \to \infty}
\end{align}
und entsprechend bei kontinuierlicher Verteilung $p(x)$ die 'Wahrscheinlichkeitsdichte'
\begin{align}
p(x) \; dx \; = \frac{dM_x}{M}
\end{align}
Normierung:
\begin{align}
\int dx \; p(x) = 1 
\end{align}
\begin{align}
\overline{x}= \int dx \; p(x) \; x
\end{align}
\textbf{Schwankung}
\begin{align}
\overline{(\Delta x)^2} = \sum_{i=1}^N p_i \left( x_i - \overline{x} \right) ^2 \widehat{=} \int dx \; p(x) \; (x- \overline{x})^2
\end{align}
häufig wichtig ist auch

\begin{align}
\overline{(x-\overline{x})^2} = \overline{x^2 - \underbrace{2 \overline{x} x}_\text{=0, s.u.} - \overline{x}^2} = \overline{x^2} - \overline{x}^2
\end{align}  \footnote{mittelt sich zu Null, da nach oben gleiche Abweichung wie nach unten} \\
\textbf{Beispiel:}(Gleichverteilung) 
\begin{align*}
p(x) = 
\begin{cases}
1  \quad \quad 0 \leq x \leq 1 \\
0  \quad \quad  \text{sonst}
\end{cases}
\quad \Rightarrow \overline{x}= \int_0^1 dx \; x = \frac{1}{2}
\end{align*}
\begin{align*}
\overline{(\Delta x)^2} = \int_0^1 dx \; (x-\frac{1}{2})^2 \approx  \int_0^1 dx \; (x^2-\frac{1}{4}) = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}   \label{Anmerkung}
\end{align*} \footnote{Die Wurzel aus (\ref{Anmerkung}) wäre der Wert für die Breite }
\paragraph{Erzeugung von Zufallszahlen im Rechner}
...geht nicht, da es beim Rechner, einer deterministischen Maschine, keinen Zufall gibt. $\Rightarrow$ 'Pseudo-Zufallszahlen', deterministische Kette von Zahlen $x_i$, die bestimmten Anforderungen genügen: 
\begin{itemize}
\item definierter Wertebereich, \textbf{z.B.} $ [ 0,1] $ \textit{(double)}, $[ 0: RAND\_ \; MAX]$\textit{(Integer)}
\item wohldefinierte Wahrscheinlichkeitsverteilung, gleichverteilt, \textbf{ z.B.} %TODO image doppeltes
\item lange Periode: $x_i$ ist immer periodisch denn
\begin{itemize}
\item Rechner speichert Information
\item Speicher begrenzt $\Rightarrow$ endliche Zahl von Zuständen
\end{itemize}
\item $x_i$ deterministisch $\Rightarrow$ aus \textit{einem} (gleichen) Zustand folgt immer die gleiche Kette $\Rightarrow \; x_i$ sind periodisch
\item großer Wertevorrat (nicht immer nötig) jedoch unmöglich, deshalb Periode lang,\textbf{ z.B.} 1 von N %TODO
aussuchen
\item keine Korrelation, d.h. kein 'einfacher' Zusammenhang zwischen den $x_i$, \textbf{ z.B.}
\begin{align*}
x_{i+k} = x_i 
\end{align*}
geht nicht wegen der Periode (hier wäre die Periode $k$).
\end{itemize}
\paragraph{Methode 1} 

linear konvergente Zufallsgeneratoren \footnote{Anmerkung: Der Rest bei Ganzzahldivision $mod$ $\widehat{=}$ $\% $ in C!} 
\begin{align}
& x_{i+1}= (a x_i + b)\; mod \; m \quad \\
\end{align}
 $x_i$: Integer ,  $m$: Wertebereich und $a,b$: 'magischen Zahlen'. \\
Man kann zeigen: Der Algorithmus hat keine Zyklen $< m$ wenn:
\begin{enumerate}
\item $b$ und $m$ teilerfremd
\item $a-1$ ein Vielfaches von jedem Primfaktor von $m$
\item $a-1$ ein Vielfaches von 4, falls 4 Teiler von $m$
\end{enumerate}
\textbf{Beispiele:} \begin{itemize}
\item $a= 137, \; \quad b= 187, \quad \quad m=256$
\item $ a= 1366, \quad b=150887, \; m=714025$
\end{itemize}
Mögliche Testverfahren dafür sind:
\begin{enumerate}
\item Histogramm bezüglich Gleichmäßigkeit der Verteilung: Man teilt den Wertebereich in $I$ Teilintervalle der Länge $\Delta x$. Es gilt $I \cdot \Delta x = 1$.  Ziehe N Zufallszahlen, $N_i$ im Teilintervall $i$
\begin{align}
N = \sum _{i=1}^I N_i, \quad \quad \quad \overline{N_i} = \frac{N_i}{I}= N \Delta x
\end{align}
\begin{itemize}
\item $\frac{N_i}{N}$ sollte für $N \to \infty $ gegen $\frac{N}{I}$ gehen
\item Konvergenz (Fehler) %TODO FELDER? \\
Betrachte 1. Teilintervall (als Beispiel) Wahrscheinlichkeit, dass von N Zufallszahlen $N_1$ im 1. Teilintervall sind: 
\begin{align}
p_N (N_1) = p^{N_1} (1-p)^{N-N_1} \frac{N!}{N_1 ! (N-N_1)!} \quad  \mbox{ \textbf{'Binomialverteilung'}} \\
 \mbox{ mit } p= \Delta x
\end{align}
%TODO IMAGE MISSING! S.59
Die Binomialverteilung geht in Grenzfällen in einfachere, d.h. analytische Verteilungen über: In diesem Beispiel geht sie für $N \to \infty$ und konstantes $p$ in die \textbf{ Gaußverteilung} über.
\begin{align}
p_N (N_1) = \frac{1}{\sqrt{2 \pi N p (1-p)}} e^{- \frac{(N_1 - N_p)^2}{2 N_p (1-p)}}
\end{align}
Der Beweis hierfür ist etwas komplizierter, unter Anderem mit \textsc{Stirling}-Formel $ln(N!) = N \; ln(N) - N + \frac{1}{2} \; ln( 2 \pi N)$. \\
Die Eigenschaften der \textsc{Gauß}-Formel auf einen Blick:
\begin{itemize}
\item Mittelwert: $\overline{N_1}= N_p$
\item Schwankung: $\chi ^2 = \overline{(N_1 - \overline{N_1})^2} \propto N p (1-p)$ 
\item $\Rightarrow$ relativer Fehler: $\frac{\sqrt{\chi ^2}}{\overline{N_1}} \propto \frac{\sqrt{N}}{N} \propto \frac{1}{\sqrt{N}}$ 
\end{itemize}
\end{itemize}
\item Weiteres Testverfahren: \\
Auf Korrelation prüfen. Aufeinanderfolgende Zufallszahlen werden als Punkte in einem d-dimensionalen Einheitskubus dargestellt:
\begin{align}
(x_k, x_{k+1}, \; ...\; ,\; x_{k+(d-1)}) \in \mathbb{R}^d
\end{align}
Falls Korrelation vorhanden: Punkte füllen nicht den Raum sondern liegen auf $(d-1)$-dimensionalen Ebenen. \\
\textbf{Beispiel:} $d=2$. 

Muster $\widehat{=}$ schlechter Zufallszahlengenerator. 

\item Momente testen: \\
Bilde \begin{align}
\frac{1}{N} \sum_{i=1}^N x_i^n
\end{align}
Falls zufällig verteilte $x_i$ (also gleichverteilt) gilt analytisch: \begin{align}
\overline{x_i^n} = \int _0 ^1 dx \; x_i^n = \frac{1}{n+1}
\end{align}
Dies muss auch stimmen für eine große Zahl $N$ von Zufallszahlen:
\begin{align}
\frac{1}{N} \sum_{i=1}^N x_i^n = \frac{1}{n+1}
\end{align}
\begin{itemize}
\item Es existieren zahlreiche Weiterentwicklungen des linear-konvergenten Zufallszahlen-Generators. (siehe z.B. \texttt{ rand()}), immer in C implementiert aber mit verschiedenen Methoden realisiert. 
\item bessere Zufallszahlen-Generatoren sind:
\begin{itemize}
\item Schieberegister (shift, Tanswert...TODO) Generator: Speichert Array von Zufallszahlen und berechnet die nächste Zufallszahl auf Basis mehrerer Vorgänger, \textbf{z.B} Kirchpatrick-Stall %TODO
$z[250]$ \begin{itemize}
\item $Z= Z[0] \underbrace{ \string^ }_\text{ \texttt{XOR} linewise} Z[147]$
\item dann wird Feld umsortiert
\item 250 Startwerte mit anderem Zufallszahlen-Generator %TODO 
\end{itemize}
\end{itemize}
\item seed reproduzierbar halten (wegen Reproduzierbarkeit wirtschaftlicher Ereignisse)
\item Programmbibliotheken!
\item Häufig sind anderen Verteilungsfunktionen gewünscht $\Rightarrow $ Transformationen
\end{itemize}

\end{enumerate}


\subsection{ Monte Carlo Integration}
Betrachten Integral 
\begin{align}
I= \int_a^b f(x) \; dx
\end{align}
Konventionelle nummerische Methode: \\
Zerlegung in kleine Intervalle $\Delta x$:

\begin{align}
I_n= \sum_{\nu =1}^n \; \Delta x \; f(x_\nu) \quad \mbox{ \textbf{'Rechteckregel'}}
\end{align}
für $n \to \infty \Rightarrow I_N \to I$ (falls integrierbar).

Besser ist die Trapezregel oder die von Simpson. Sie sind im Prinzip 'gleich', unterscheiden sich allerdings bezüglich ihrer Konvergenz. \\

Verallgemeinerung auf höhere Dimensionen:

d-dimensionales Integral, $n$ Unterteilungen (Stützstellen) pro Achse \\
$\Rightarrow n^d$ Hypercubi der Göße $(\Delta x)^d$ und damit \\
$\Rightarrow n^d$ Terme müssen summiert werden\\
Für kleine $d$ kein Problem, für große jedoch absolut unmöglich. \\

\textbf{Beispiel:} (Phasenraumintegral) \\
 3TL, Freiheitsgrade $\vec{r}, \vec{p} \; \quad \Rightarrow$ Integral im Phasenraum: 

\begin{align*}
\int d^3 \vec{r}_1 d^3 \vec{r}_2 d^3 \vec{r}_3 \; d^3 \vec{p}_1 d^3 \vec{p}_2 d^3 \vec{p}_3 .... 
\end{align*} %TODO bild
ist immerhin 18-dimensional mit 
\begin{align*}
 n=100, d=18 \Rightarrow n^d=100^{18}=10^{36} \mbox{ Summanden (Rechenoperationen)}
\end{align*} 
Unsere Cluster schaffen $10^{10}$FLOPS
$\Rightarrow$ Summation dauert $10^{36 } / 10^{10}= 10^{26}s$ \\ (Vergleich: alter des Universums ist $10^{19}s$!) \\
Wir brauchen also ganz andere Methoden für hochdimensionale Integrale - die Statistik! 

Dafür betrachten wir wieder (der Einfachheit halber) folgendes Integral:
\begin{align}
I= \int_a^b f(x) \; dx
\end{align}
mit $x_1,....x_N$ als $N$ gleichverteilte Zufallszahlen über $[a,b]$. %TODO bild
Wir zerlegen das Intervall in $n$ Kästchen der Länge $\Delta x$.
$N_\mu$ sei weiterhin die Anzahl der $x_i$ im $\nu$ten Teilintervall. Dann gilt
\begin{align}
n \cdot \Delta x =&\;  b-a \; \quad
\Rightarrow \frac{\overline{N}_\nu}{N}= \frac{1}{n}= \frac{\Delta x}{b-a} \\
 \Rightarrow I= & \int_a^b f(x) \; dx  \approx  \sum_{\nu =1}^N \Delta x \; f(x_\nu )  \quad \quad \mbox{(Rechteckregel)} \\
 =& \sum_{\nu =1}^n (b-a)  \frac{\overline{N}_\nu}{N} f(x_\nu) 
= \frac{b-a}{N} \sum_{\nu =1}^n \underbrace{\overline{N}_\nu f(x_\nu)}_{ \approx \sum_{x_j \in [\,]_\nu} f(x_j)} \\ =& \frac{b-a}{N} \sum_{i=1}^N f(x_i)
\end{align}
Diese Formel ist nicht auf $d=1$ beschränkt, sondern gilt auch in höheren Dimensionen:
\begin{align}
\Aboxed{ I \approx \frac{V_d}{N} \sum_{i=1}^N f(\vec{x}_i) }
\end{align}
d.h. das Integral ist der Mittelwert der Funktionswerte an zufälligen Stützstellen $\vec{x}_i$.\\
\paragraph{Fehler:} (für $I = \int_0^1 f(x) dx$)\\
$\{ x_1, x_2,..., x_N\}$ sei spezielle Folge von Zufallszahlen mit $I_1$ als Integralwert (also Resultat). Eine andere Folge erzeugt anderes Ergebnis $I_2,...$. \\
$\Rightarrow$ Es existiert eine Wahrscheinlichkeitsverteilung $P(I)$ um dafür einen bestimmten Integralwert $I$ zu erhalten.   %TODO image
Betrachte deswegen unendliche viele Ketten\footnote{$\to$ exaktes Resultat, aber wie groß ist dann die Schwankung?} von Zufallszahlen der Länge $N$:\\
$\Rightarrow P(I) dI$ wäre dann die Wahrscheinlichkeit, dass $I$ im $I$-ten Intervall $dI$ um $I$ liegt.
\begin{align}
\langle I \rangle =& \int dI \; P(I) \; I \; \quad \quad \left( \mbox{ mit } I= \frac{1}{N} \sum_{i=1}^N f(x_i)= I(x_1,x_2,...,x_N) \right) \\
=& \int dx_1,...,dx_N \; \rho(x_1) ... \rho(x_N) \; \frac{1}{N} \sum_{i=1}^N f(x_i) \\
=& \int_0^1 dx \; f(x) 
\end{align}
unendlich viele Folgen von Zufallszahlen der Länge $N$ liefern gemittelt das exakte Ergebnis! Die Berechnung des Fehlers folgt dann:
\begin{align}
\langle (I- \langle I \rangle)^2 \rangle =& \langle I^2 \rangle - \langle I \rangle ^2 \\
=& \langle \frac{1}{N^2} \left( \sum_{i=1}^N \; f(x_i) \right) ^2 \rangle - \langle \frac{1}{N} \sum_{i=1}^N \; f(x_i)\rangle^2 \\
=& \frac{1}{N^2} \sum_{i,j=1}^N \langle f(x_i) \; f(x_j) \rangle - \frac{1}{N^2} \sum_{i,j=1}^N \langle f(x_i) \rangle \; \langle f(x_j) \rangle \\
=& \frac{1}{N^2} \sum_{i=1}^N \langle f(x_i)^2 \rangle - \frac{1}{N^2} \sum_{i=1}^N \langle f(x_i) \rangle ^2 \; +\mathcal{O} \quad \mbox{ für } i = j
\end{align}

weil
\begin{align*}
\langle f(x_i) \; f(x_j) \rangle = \langle f(x_i) \rangle \; \langle f(x_j) \rangle \; \mbox{ für } i \neq j
\end{align*}

brauche
\begin{align}
\int dx_1 dx_2 \; & \rho(x_1) \rho(x_2) \; f(x_1) f(x_2) \\
&= \int dx_1 \rho(x_1)f(x_1)  \underbrace{\int dx_2 \rho(x_2)}_\text{=1}\times  \int dx_2 \rho(x_2)f(x_2) \cdot \underbrace{\int dx_1 \rho(x_1)}_\text{=1} 
\end{align} 
aber
\begin{align*}
\int dx_1 \; \rho (x_1) f(x_1)^2 \neq \left( \int dx_1 \; \rho (x_1) f(x_1) \right) ^2
\end{align*}

\begin{align*}
 \Rightarrow \langle \Delta I \rangle ^2= \frac{1}{N} \left( \langle f(x)^2 \rangle - \langle f(x) \rangle^2 \right)
\end{align*}
\paragraph{Diskussion:} \begin{itemize}
\item[1)] Konvergenz:
\begin{align}
\Delta I \propto \frac{1}{\sqrt{N}}
\end{align}
Zum Vergleich der Konvergenz mit herkömmlichen Methoden, \textbf{z.B.} der Trapezregel. Ihr Fehler in einer Dimension ist $\propto h^2 $. $N$ Rechenschritte: $h \propto N^{ \frac{-1}{d} }$ in $d$ Dimensionen. $\Rightarrow \Delta I \propto N^{\frac{-2}{d}}$\\
$ \Rightarrow$ für $d<4$ ist Monte Carlo 'besser' als Trapezregel

\item[2)] $I \to \langle I \rangle$ für $N \to \infty$. \textit{Eine} $\infty$-lange Kette (o. Folge) führt (auch) zum exakten Resultat, d.h. das Verfahren ist sebstmittelnd.

\item[3)] Verfahren gut, wenn $f$ möglichst konstant.  Anschaulich in Abbildung . Man sieht, dass das Verfahren exakt ist, falls $f= const.$ %TODO bild
\item[4)]Der Fehler lässt sich während der Integration berechnen und sollte endlich bleiben.
\end{itemize}
\textbf{Beispiel:} (Volumen einer D-Dimensionalen Einheitskugel) \\
Wir starten in 2D. %TODO bild
\begin{align*}
f(x,y)=
\begin{cases}
1 & x^2+y^2 <1 \\
0 & \mbox{sonst}
\end{cases}
\end{align*}
Flächenberechnung: 
\begin{align*}
I= \int_0 dx \; dy \; 1 = \int_{-1}^1 dx \int_{-1}^1 dy \; f(x,y) 
\approx \frac{4}{N} \sum_{i=1}^N f(x_i,y_i)
\end{align*}
mit $[x_i, y_i]$ gleichverteilt aus $[-1,1]$.
Zeige Programm: Das Volumen verschwindet für hohe Dimensionen: Eindimensionale Betrachtung: Intervall geht exakt von $[-1,1]$, bei einem Kreis in $[-1,1] \times [-1,1]$ fallen schon die Ecken weg. Das Volumen auf den Einheitsradius ist nicht mehr ganz so groß. Bei drei Dimensionen fallen schon die 8 Ecken weg, daher nimmt das relatives Volumen für große Dimensionen ab. \\
$\to$ Wie kann man das Verfahren verbessern?

\paragraph{Verbesserung:}
\begin{align}
(\Delta I )^2 = \frac{1}{N} \underbrace{\left( \langle f^2(x) \rangle - \langle f(x)\rangle^2 \right)}_\text{Schwankungsbreite der Fkt}
\end{align}
Idee: Transformation, so dass der Integrand $\approx const$, dafür aber Zufallszahl \textit{nicht} gleichverteilt.
\begin{align}
\Aboxed{ I= \frac{1}{N} \sum_{i=1}^N \frac{f(y_i)}{w(y_i)} }
\end{align}
wobei $y_i$ eine Folge von Zufallszahlen gemäß Verteilung $w(y)$ mit $w(y)>0$, $\int w(y) \; dy=1$. \\
Beweis:
\begin{align}
\langle I\rangle = \langle \frac{1}{N} \sum_{i=1}^N \frac{f(y_i)}{w(y_i)} \rangle = \frac{1}{N} \sum_{i=1}^N \langle \frac{f(y_i)}{w(y_i)} \rangle = \frac{1}{N} \sum_{i=1}^N \int dy_i \; w(y_i)  \frac{f(y_i)}{w(y_i)} 
= \int dx \; f(x)
\end{align}
Fehler: \begin{align}
\Delta I=\frac{1}{\sqrt{N}}
 \sqrt{\langle \left(\frac{f}{w}\right)^2 \rangle - \langle \frac{f}{w} \rangle ^2}
 \end{align}
 $\Rightarrow$ für kleine Felder: $w$ so wählen, dass $\frac{f}{w} \approx const.$ Man muss allerdings $w$ integrieren können und $w$ muss $f$ ähnlich sein und man braucht Zufallszahlen mit einer vorgegebenen Verteilung! %TODO grafik \\
 
 $\Rightarrow$ Mann nennt dieses Verfahren:\textbf{Importance sampling}, (weil 'wichtige'$\widehat{=}$ 'hohe' Werte des Integranden $f(x)$ in der ursprünglichen Funktion häufiger genommen werden, im Gegensatz zum herkömmlichen \textbf{simple sampling})\\ $\Rightarrow$ die Wurzel N im Fehler ist geblieben. Leider konnten wir die Konvergenz dadurch nicht verstärken. Neues Problem hierbei ist jetzt: 
 
 \subsection{Zufallszahlen einer vorgegebenen Verteilung}
 Problem: %TODO grafik \\
 Zufallszahlen $x_i$ aus einem Intervall berechnen mit einer Verteilung $\rho(x)$ mit
 \begin{itemize}
 \item $\rho(x)>0$
 \item $\int_0^1 dy \rho(x)=1$.
\end{itemize} 
 
 \begin{itemize}
 \item[a)] \textbf{Rejection-Method} nach \textsc{von Neumann} (1947) \\
 %TODO grafik
 betrachte Paar von Zufallszahlen, $x_i \in [0,1], y_i \in [0,b]$ mit $b=Max(\rho(x))$.
 
$\to$ wenn $y_i < \rho(x_i) \Rightarrow x_i$ wird akzeptiert mit $\xi_i=x_i$\\
 $\to$ wenn $y_i > \rho(x_i) \Rightarrow x_i$ wird nicht akzeptiert. \\
 
 $\Rightarrow$ Folge von Zufallszahlen $\xi_i$. Zahl der $\xi_i \in \Delta x$ ist proportional zur Fläche $\rho(x) \cdot \Delta x$ und damit proportional zu $\rho(x)$.
 
 \textbf{Problem:} viele Züge notwendig, wenn selten akzeptiert wird. 
 
 \textit{Schlecht wäre zum Beispiel:} die Betrags-Exponentialfunktion (\textsc{Gauß}funktion) benötigt ein großes $x-$Intervall.
 %TODO image
 
\textit{Gut wäre dafür aber:} Kugeln - 
Vektoren innerhalb (oder auf) Einheitskreis
%TODO

\item[b)] \textbf{Transformationsmethode} \\
betrachte monotone Funktion. Dabei seien wieder $x_i$ gleichverteilte Zufallsvariablen aus $x_i \in [0,1]$ und $y_i = f(x_i)$.Frage: Wie sind die verteilt? \footnote{Dafür muss man die Wahrscheinlichkeiten umrechnen. In ein $\Delta x$ fallen irgendwelche Zufallsvariablen rein und werden auf $\Delta y$ abgebildet, das ja kleiner sein kann. Die dichte in $\Delta y$ sowie $\Delta x$ kann also verschieden sein.} %TODO siehe image

$N$ Zufallszahlen \\
 $\Rightarrow$ $N \cdot \Delta x$ fallen in das Intervall $\Delta x$. Die entsprechenden abgebildeten Zufallszahlen $y_i=f(x_i)$ fallen in $\Delta y$ um $f(x)$.

$\Rightarrow$ Änderung der Punktdichte $\rho(y)\Delta y = \Delta x$. \\
Der limes $\Delta x \to 0$ mit 
\begin{align*}
\rho(y)=\frac{dx}{dy}
\end{align*}
soll vorgegeben werden. Wie lautet dann $f(x)$?
\begin{align}
\Rightarrow  \int_0^x dx' = \int \rho(y) dy \; \Rightarrow \; x(y) = \int \rho(y) \; dy = x(y) = f^{-1}(y) 
\end{align}
\begin{align}
\Aboxed{\Rightarrow  f(x)= \left( \int \rho(y) \; dy \right)^{-1} }
\end{align}
\begin{align*}
\Rightarrow  f(x_i)=y_i \mbox{ sind gesuchte ZZ}
\end{align*} 
also: $\rho(y)$ gegebene Verteilung muss man 
\begin{enumerate}
\item[1)] Integrieren
\item[2)] Invertieren
\end{enumerate}

\textbf{ Beispiel:}
 \begin{align*}
  \rho(y)=
 \begin{cases}
e^{-y} & \text{für } y \geq 0 \\
0 & \text{für } y < 0.
 \end{cases}
 \end{align*}
 und wie gewünscht 
 
 \begin{itemize}
	 \item Normierung 
  		$\int_0^\infty  \rho(y) \; dy = 1 $
 	\item Positiv
 		$\rho(y) \geq 0 $
\end{itemize}

 \begin{enumerate}
 \item Es gilt: \begin{align*}
 \int_0^y dy' \rho(y') = 1- e^{-y} \equiv x(y)
 \end{align*}
 \item Umkehrfunktion:
  \begin{align*}
 y= - ln(1-x) = f(x)
 \end{align*}
 $\Rightarrow$ ziehe gleichverteilte Zufallszahl $x_i \in [0,1)$ (nicht die 1 selber!) \\
 $\Rightarrow y_i = -ln(1-x_i)$ sin exponentiell verteilte Zufallszahlen $\in [0,\infty]$.
 \end{enumerate}
 Nachteil: 
\footnote{Der Computer ist nicht so schnell beim logarithmieren...deswegen ist die rejection Methode diesbezüglich interessanter.}
Man muss die gewünschte Verteilung $\rho(y)$ erst integrieren und dann zusätzlich auch invertieren können. Dies geht nicht bei \textbf{z.B.} der \textsc{Gauß}-Verteilung

\item[c)] \textbf{Gauß-Verteilung } (Normalverteilung) \\
\begin{align*}
\rho(x)= \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^2}{2 \sigma^2}}
\end{align*}
Wieder gilt:
 \begin{itemize}
	 \item Normierung 
  		$\int_0^\infty  \rho(y) \; dy = 1 $
 	\item Positiv
 		$\rho(y) > 0 $
\end{itemize}


Sie ist nicht analytisch (bestimmt) integrierbar, sodass die Transformationsmethode nicht anwendbar ist. Dafür gibt es aber einen Trick: \\
betrachte eine zweidimensionale Wahrscheinlichkeitsverteilung
\begin{align}
p(x_1,x_2)=\frac{1}{	2 \pi \sigma^2} e^{- \frac{x_1^2 + x_2^2}{2 \sigma^2}} = \rho(x_1) \cdot \rho(x_2)
\end{align}
die Zahl der Punkte im Intervall $dx_1, dx_2$ ist:
\begin{align}
\frac{1}{	2 \pi \sigma^2} e^{- \frac{x_1^2 + x_2^2}{2 \sigma^2}} dx_1 dx_2= & \frac{1}{	2 \pi \sigma^2} e^{- \frac{r^2}{2 \sigma^2}} \;r \; dr  d\phi \quad \left( \mbox{subst: } u = \frac{r^2}{2 \sigma^2} \right) \\
= & \frac{1}{2 \pi} e^{-u} du \; d\phi
\end{align}
mit
\begin{align*}
x_1=  r cos(\phi) = \sigma \sqrt{2 u} \; cos(\phi) \\
x_2=  r sin(\phi)= \sigma \sqrt{2 u} \; sin(\phi)
\end{align*}
also...
\begin{itemize}
\item[...] ziehe: 
\begin{itemize}
\item[-] Zufallszahl $\phi_i$ aus $[0,2\pi]$
\item[-] Zufallszahl $y_i$ ist exp-verteilt aus $[0, \infty]$
\end{itemize}
\item[...]rechne:
\begin{itemize}
\item[-] $x_i = \sigma \sqrt{2 u_i} \; cos(\phi)$
\item[-] $x_i' = \sigma \sqrt{2 u_i} \; sin(\phi)$
\end{itemize}
\end{itemize}



praktisch: \begin{align}
\Aboxed{x_{2i}= \sigma \sqrt{-2 ln(1-y_{2i})} \; cos(2\pi \; y_{2i-1}) } \\
\mbox{ mit } y_i\mbox{ ZZ} \in [0, 1) \mbox{ gleichverteilt} \nonumber
\end{align}
Beispiel: Wir nehmen das Integral
\begin{align*}
\int_0^{2\pi} x \; e^x \; dx \underbrace{=}_\text{analytisch} e^{2\pi} (2\pi -1) +1
\end{align*} 
Importance: \begin{align*}
\rho(y)= \frac{e^y}{e^{2\pi} -1}\quad  \left( \mbox{normiert } \int_0^{2\pi} e^y \; dy =1 \right) 
\end{align*}
\begin{align*}
\int_0^y \rho(y') \; dy' = \frac{e^y - 1}{e^{2\pi} - 1} = x(y)
\end{align*}
\begin{align*}
 \Rightarrow y= ln(\underbrace{(e^{2\pi}-1)}_\text{Norm.-Konst} x+1) 
 \end{align*}
 \begin{align*}
\Rightarrow  I = \frac{1}{N} \sum_{i=1}^N \frac{f(y_i)}{\rho(y_i)}= \frac{1}{N} \sum_{i=1}^N y_i (e^{2\pi}-1)
\end{align*}
wobei $x_i \in [0,1)$.
\end{itemize}

 
% %TODO Review:
% Importance sampling: \begin{equation}
% i= \frac{1}{N} \sum_{i=1}^N \frac{f(y_i)}{w(y_i)} \mbox{ mit } y_i: \mbox{ Zufallszahl normalverteilt }
% \end{equation}
% 
%\textbf{ Beispiel: }
% \begin{align}
% \int_0^{2\pi} x \; e^x \; dx = e^{2\pi} (2\pi -1) +1\\
% \rho (y)= \frac{e^y}{e^{2\pi}-1} \\
% y= ln \left( (e^{2\pi} -1)\; x \; +1\right) \mbox{ mit } x \in [ 0,1] \\ \Rightarrow I= \frac{1}{N}\sum_{i=1}^N y_i (e^{2\pi}-1)
% \end{align}
% %TODO Review END
 
 \subsection{Ising Modell}
 Das \textsc{Ising}modell\footnote{Zweizustandsmodell: minimales Modell für ein reales system, jeder einzelne Freiheitsgrad hat nur zwei Zustände, ist aber das erste Modell das einen Phasenübergang zeigt. Es ist im $2d$ Fall aber exakt lösbar und daher gut für uns.} findet zahlreiche Anwendungen (\textbf{z.B.} in den Sozialwissenschaften, in der Phyik für Fest-Flüssig-Übergänge oder mehr noch für \textit{magnetische Eigenschaften}, wie etwa dem Ferromagneten). Es ist \textit{sehr einfach} und zudem auch \textit{exakt lösbar} für Lösungen im $1d$ (\textsc{Ising}), $2d $ (\textsc{Onsager}) Fall.
($\to$ Nobelpreis) \\
Wir benutzen das Modell eines Ferromagneten:  \\
Makroskopisch ist eine Gesamtmagnetisiertung zu beobachten. Auf mikroskopischer Ebene findet man Atome mit 'Spin', atomare Momente, die in eine Richtung ausgerichtet sind. Sie wechselwirken mit ihren umliegenden Nachbarn\footnote{ Die Wechselwirkung ist im einfachsten Fall nur mit den nächsten Nachbart, idee: überlapp der wellenfunktion ist begrenzt exponentiell}. Es gibt Vorzugsrichtungen, abhängig von den Kristallstrukturen (Wegen Spin-Bahn Wechselwirkungen) \\
$\Rightarrow$ Vorzugsachse (\textbf{Anisotropie}). Der Kristall hat eine leichte Achse, d.h. Spins wollen in diese Richtung stehen.\\
$\Rightarrow$ nur noch zwei Zustände! ('rauf' $\uparrow$ oder 'runter' $\downarrow$) \\

 %TODO image

 \begin{itemize}
\item[-] Gitter aus Spins, im einfachsten Fall $S= \pm 1$, sodass beispielsweise $S_i$ nur index $i= \{+,-\}$ hat.
\item[-]Hamilton-Funktion\footnote{Spin ist hier kein Drehimpuls sondern wirklich ein Spin, das Minuszeichen kommt vom Drehimpuls des \textit{Elektrons}. Es ist ein phänomenologisches klassisches Modell, also nicht wirklich ein quantenmechanisches Spin-$1/2$-Modell sonder im klassischen Limes (Limes Heisenbergmodell und gleichzeitig Anisotropie gegen unendlich)}
\begin{align}
H= - \sum_{i=1}^N B \; S_i
\end{align}
\begin{itemize}
\item[-] $B> 0 : \; S_i = 1$ günstig 
\item[-]  $B<0 : \; S_i = -1$ günstig 
\item[-] $B=0; \; S_i = \pm 1$ unmöglich
\end{itemize}
 \item[-]Hinzüglich einer \textbf{Wechselwirkung:} Überlapp der Wellenfunktion führt zu Austauschenergie $J$ zwischen denjenigen Spins, die nächste Nachbarn sind. (Dies ist nicht die makroskopische (Dipol-)Wechselwirkung) 
 
 \begin{align}
  H(S_1, S_2, ..., S_N) = \underbrace{ -J \sum_{i,j=1;\; i,j NN}^N S_i S_j }_{\substack{\text{Nachbarn auf dem Gitter} \\ \text{jedes Paar einmal}}} - B \sum_{i=1}^N S_i
\end{align}  
($B$ gibt vor, welche Ausrichtung energetisch günstiger ist. Beispiel: $\uparrow \uparrow, B: \Uparrow$)
  \begin{itemize}
  \item $J>0: \; \uparrow \uparrow  \quad E= -J$ günstig $\rightarrow$ Ferromagnet
  \item $J<0: \; \uparrow \downarrow  \quad E= -J$ günstig $\rightarrow$ eventuell Antiferromagnet ($\uparrow \downarrow \; , \; \downarrow \uparrow$)
  \end{itemize}
  Im Folgen $J>0$. 

\begin{enumerate}
\item Grundzustand (Kette): $E_0= -J(N-1)-BN $. $\uparrow \uparrow \uparrow \uparrow \uparrow \uparrow$
\item Endliche Temperatur $T$: angeregte Zustände kommen vor, z.B. $\uparrow \uparrow \uparrow \downarrow \uparrow$ \textit{(erster angeregter Zustand)}
\end{enumerate}

\textbf{Beispiel:} System mit abzählbar vielen (quantisierten) Zuständen.  
\begin{align*}
N(4) \mbox{ Spins, } S= \frac{1}{2} \quad \quad \Aboxed{ \uparrow \downarrow \uparrow \uparrow }
\end{align*}
Wie berechnet man nun das \textbf{thermische Mittel} einer Observablen?
\begin{align}
\langle \Omega \rangle= \sum_{ \{\bar{S} \}}  p(\{ S \}) \; \Omega(\{S\})
\end{align}
Dabei ist $\{ S \}$ ein Zustand des Systems und $\Omega(\{ S \})$ die zu diesem Zustand gehörende Größe. Die Wahrscheinlichkeit $p(\{ S \})$ meint die Wahrscheinlichkeit, dass das System in diesem Zustand ist. Eine vereinfachte Schreibweise ist
\begin{align*}
\{ S \} \; \widehat{=} \; \underline{S}
\end{align*}
Wir nummerieren die Zustände durch
\begin{align}
\langle \Omega \rangle = \sum_{\underline{\dot{S}}} p_{\underline{\dot{S}}} \; \Omega_{\underline{\dot{S}}}
\end{align}
wobei man zeigen kann dass:

%\item[-] kanonische Gesamtheit 
\begin{align}
p_{\underline{\dot{S}}} = p ( H_{\underline{\dot{S}}}) = \frac{e^{- \beta H_{\underline{\dot{S}}} }}{\underbrace{ 
\sum_{\underline{\dot{S}}}  e^{-\beta H_{\underline{\dot{S}}} } 
}_{\text{Normierung:} \sum_{\underline{\dot{S}}} p_{\underline{\dot{S}}} =1 }}\; , \quad \beta = \frac{1}{k_BT}
\end{align}
mit \textit{Zustandssumme} 
\begin{align}
\Aboxed{ Z= \sum_{\underline{\dot{S}}} e^{- \beta H_{\underline{\dot{S}}} } }
\quad
\mbox{ und }  \Aboxed{ \langle \Omega \rangle = \frac{1}{Z}  \sum_{\underline{\dot{S}}} \Omega({\underline{\dot{S}}}) \;  e^{- \beta H_{\underline{\dot{S}}} }}
\end{align} (fürs kanonische Ensemble bestimmbar in obiger Form)
\end{itemize}
Das \textsc{Ising}-Modell ist lösbar in $1d$ und $2d$. Wir beobachten im Folgenden aber Numerik und deswegen analytisch lösbare 'Vorüberlegung' mit nur 2 Spins:\\
\textbf{ Beispiel:} ($N=2$): 
Die beobachtbare Magnetisierung 
\begin{align}
M = \frac{1}{N} \sum_{i=1}^N S_i = \frac{1}{2} (S_1 + S_2)
\end{align}
Wieder betrachten wir den thermischen Mittelwert bei Ankopplung der Spins an ein Wärmebad:
\begin{align}
\langle M \rangle = \frac{1}{Z} \sum_{ \{ S_1, S_2 \} } M \; e^{- \beta \;  H(S_1,S_2)} \quad \left(\mbox{ mit: } Z = \sum_{ \{ S_1, S_2 \} }  e^{- \beta \; H(S_1, S_2)} \right)
\end{align}
im Allgemeinen haben wir
\begin{align*}
2^N \mbox{ Zustände } \uparrow \uparrow \; \downarrow \uparrow \; \uparrow \downarrow \; \downarrow \downarrow
\end{align*}
aus $2$ Spins folgen also 4 Zustände.

\begin{align*}
Z= e^{-\beta (-J -2B)} + 2 e^{-\beta J} + e^{- \beta (-J +2B)}
\end{align*}
\begin{align*}
 M &= \frac{1}{2} \frac{2 e^{-\beta (-J - 2B)} - 2 e^{-\beta (-J + 2B)}}{Z} = \frac{e^{\beta J} \left( e^{2 \beta B} - e^{-2\beta B} \right)}{e^{\beta J} \left( e^{-2\beta B} + e^{-2 \beta B} + 2 e^{-2 \beta J} \right)} \\
 &= \frac{sinh( 2 \beta B) }{(cosh(2 \beta B) + e^{-2 \beta J}}
\end{align*}
 
%TODO image
Allgemein: N Spins \\
$\Rightarrow \; 2^N$-Zustände ($2^{100} = 10^{30}$) \\
$\Rightarrow$ d.h. durch direktes Summieren numerisch nicht lösbar \\
$\Rightarrow$ Näherungsverfahren!

\subsection{Monte Carlo Simulation}
(Wir rechnen im kanonischen Ensemble die Zustandssumme aus.)
\begin{align}
\langle M \rangle = & \frac{\sum^{2^N}_{\underline{S}} M(\underline{S}) \;  e^{-\beta H(\underline{S} ) }}{\sum^{2^N}_{\underline{S}} e^{-\beta H(\underline{S})}}  
\begin{cases}
 \underbrace{\approx}_{\substack{ \text{simple} \\ \text{sampling}}} 
& \frac{\sum^{k}_{\underline{S}} M(\underline{S}) \; e^{-\beta H(\underline{S})}}{\sum^{k}_{\underline{S}} e^{-\beta H(\underline{S})}} \mbox{ nicht alle } 2^N \mbox{Zust. sondern } k \\
\underbrace{\approx}_{\substack{ \text{importance} \\ \text{ sampling}}} & 
\frac{\sum^{k}_{\underline{S}} M(\underline{S}) \; e^{-\beta H(\underline{S})} \frac{1}{w(\underline{S})}}{\sum^{k}_{\underline{S}} e^{-\beta H(\underline{S})} \frac{1}{w(\underline{S})}}
\end{cases}
\end{align}

Dabei sind $[\underline{S}]$ Konfigurationen von Spins mit Wahrscheinlichkeit $w(S)$. Wähle 

\begin{align}
\Aboxed{ w(\bar{S})= e^{-\beta H(\underline{S})}  }
\end{align}
 und dann folgt

\begin{align}
\Aboxed{ \langle M \rangle = \frac{1}{K} \sum_{ [\underline{S}]}^{(k)} M(\bar{S})}
\end{align}
$\Rightarrow$ Problem: wir brauchen Zustände $\overline{S}$ mit Wahrscheinlichkeit $w(\overline{S})\propto e^{-\beta H(\overline{S})}$ ($\Rightarrow$Problematisch). Wir brauchen also ein Verfahren das Zustände des Systems (Spinkonfiguration $\underline{S}$) erzeugt die eben dieser eben genannten Wahrscheinlichkeitsverteilung genügt. Wie bekommt man jetzt Konfigurationen mit einer bestimmten Wahrscheinlichkeitsverteilung? 
Lösung: \textbf{ Metropolis-Algorithmus}. \\

Beginne einen Markov-Prozess (Kette)\footnote{meint dass der nächste Zustand nur von seinem Vorgänger abhängt, d.h. ein Zustand wird vom Vorgängerzustand erzeugt} 
\begin{align*}
\underline{S}_0 \rightarrow \underline{S}_1 \rightarrow \underline{S}_2
\end{align*}
(also beispielsweise $\uparrow \uparrow \uparrow \; \rightarrow \; \uparrow \downarrow \uparrow \; \rightarrow \; \uparrow \downarrow \downarrow$)

\begin{enumerate}
\item $\underline{S}_n $ sei ein Zustand
\item \label{Punkt2} erzeuge Versuchszustand \textit{(trial state)} $\underline{S}_r$ durch \textit{geeignete} Veränderung.
\item berechne: 
\begin{align}
r= \frac{w(\underline{S}_r)}{w(\underline{S}_n)} = \frac{e^{- \beta H(\underline{S}_r )}}{e^{- \beta H(\underline{S}_n )}}= e^{-\beta (H(S_r)-H(S_n))}
\end{align}

\item Fallunterscheidung: \\ 
$r>1$: akzeptieren, $\underline{S}_{n+1} = \underline{S}_r$\\
$r \leq 1:$ akzeptieren mit Wahrscheinlichkeit $r$
\item  $\rightarrow$ \ref{Punkt2}.
\end{enumerate}
\textbf{Implementierung:} am Beispiel einer Spinkette $\uparrow \uparrow \uparrow \uparrow \uparrow \uparrow$ (der letzte wechselwirkt wieder mit dem ersten wieder, periodische Randbedingungen also)

\begin{enumerate}
\item Array von Spins \texttt{int spins}$[N]$: $ +1|+1|+1|+1|+1|+1|$ (mit Anfangsbedingung $S_i =1$)
\item \label{wiederPunkt2} Versuchsschritt: misst \textbf{'single spin flip'}, d.h. ein Spin wird gedreht: \\
$S_i \rightarrow - S_i$: $ +1|-1|+1|+1|+1|+1|$
\item $\Delta H= H(\underline{S}_r)  - H(\underline{S}_n) = 2 J (S_i S_{i-1} + S_i S_{i+1}) + 2BS_i \; \; \Rightarrow r= e^{-\Delta H / k_BT}$
\item \texttt{if (rand()/Randmax $<$r)} $S_r \rightarrow -S_i$
\item $\rightarrow$ \ref{wiederPunkt2}.
\end{enumerate}

\begin{itemize}
\item Wenn alle Spins einmal abgefragt wurden: $1MCS$ (\textbf{M}onte \textbf{C}arlo \textbf{S}chritt) pro Spin
\item Mittelung über viele MCS

\item Zu Beginn der Simulationen ist der Markov Prozess nicht im Gleichgewicht (hängen vom Anfangszustand ab) \\
$\Rightarrow$  die ersten $k$ MCS sollten nicht zur Mittlung herangezogen werden und damit bei der Berechnung von $\langle M \rangle$ weggelassen werden.
 \begin{align}
\langle M \rangle = \frac{1}{(K-k)} \sum_{i=k}^K M_i 
\end{align}
mit der Magnetisierung $M_i$ des $i-$ten Spinlaufs
\begin{align*}
M_i = \frac{1}{N} \sum_{j=1}^N \sigma
\end{align*}
\textbf{Beweis:} Metropolis-Algorithmus erzeugt Konfigurationen $\underline{S}$ mit einer Wahrscheinlichkeitsverteilung von $w(\underline{S}) \propto e^{-\beta H(\underline{S})}$ (Wie betrachtet man denn nun statistische nicht Gleichgewichtsprozesse so wie diesen Markov Prozess?)
\item $w(\bar{S}):$ Wahrscheinlichkeit im Zustand $\underline{S}$ zu sein.
\item Markov $\underline{S} \rightarrow \underline{S'}$ mit $p(\underline{S}, \underline{S'}): $ Wahrscheinlichkeit, im Prozess von $\underline{S}$ nach $\underline{S'}$ zu wechseln

Aufgabe: $p$ bestimmen, sodass $w(\underline{S})$ herauskommt
\end{itemize} %TODO bild mit strichen
\begin{align*}
\Delta w(\underline{S}) =& 
- \sum_{\underline{S'}} w(\underline{S}) \; p(\underline{S} \rightarrow \underline{S'})\quad  \mbox{ \textit{(raus)} } \\
 &+  \sum_{\underline{S'}} w(\underline{S'}) \; p(\underline{S'} \rightarrow \underline{S}) \quad \mbox{\textit{ (rein)} } \\ 
 \overset{!}{=}& 0
\end{align*}

%TODO ising modell: phasenübergang bei 2....irgendwas. Schwankungen in der reduzierten magnetisierung für werte kurz unter 2 kommen durch spezifische Wärmekapazität als schwankung der Energie und genauso die spezifische suszeptibilität als schwankung der Magnetisierung. Die zahl der Montecarlo schritte steigt bis man im gleichgewicht ankommt für steigende werte für T. Sie fällt für höhere werte detulich größer aus bis sie bei Null ankommt

Betrachte eine mögliche Lösung: jeder einzelne Summand wird $=0$. (\textbf{'detailed balance'}).
 \begin{align}
\Rightarrow   w(\underline{S}) \; p(\underline{S} \rightarrow \underline{S}')- w(\underline{S}') \; p(\underline{S}' \rightarrow \underline{S}) =0 
\end{align}
\begin{align}
\Rightarrow  \frac{p(\underline{S} \rightarrow \underline{S}')}{p(\underline{S}' \rightarrow  \underline{S})} = \frac{w(\underline{S}')}{w(\underline{S})} = e^{-\beta(E(\underline{S}')- E(\underline{S})}
\end{align}
\begin{enumerate}
\item \underline{Lösung:} (eigentl. Metropolis Algorithmus)
 \begin{align}
p(\underline{S} \rightarrow \underline{S}')
\begin{cases}
e^{-\beta(E(\underline{S}')- E(\underline{S})} & \text{ für } \Delta E > 0 \\
1 & \text{ sonst}
\end{cases}
\end{align} 

\item \underline{Lösung:} (\textbf{Heat-bath algorithm})
\begin{align*}
p(\underline{S} \rightarrow \underline{S}') = \frac{1}{1+ e^{\beta \Delta E}} \mbox{, weil } 
\end{align*}

\begin{align*}
\frac{p(\underline{S} \rightarrow \underline{S}')}{p(\underline{S}' \rightarrow \underline{S})} = \frac{1+e^{-\beta \Delta E} }{1+e^{\beta \Delta E}} = \frac{e^{-\beta \Delta E} (e^{\beta \Delta E}+1)}{1+e^{\beta \Delta E}}= e^{-\beta \Delta E}
\end{align*}
%TODO image

\end{enumerate}
\subsection{Master Gleichung und Monte Carlo Dynamik}
Zusammenhang mit irreversibler Dynamik.

betrachte System im nicht-Gleichgewicht, das equilibriert (irreversibel) %TODO image
\begin{itemize}
\item Stellen uns vor, wir haben $\infty$ viele Kopien des Systems
\item Berechne nun die zeitliche Änderung der Wahrscheinlichkeit $p_r(t)$ (Wahrscheinlichkeit, das System zur Zeit $t$ im Zustand $r$ zu finden)
\item betrachte quantenmechanisches System im Kontakt mit einem Wärmebad
\begin{align}
\bar{H}_{gesamt} = \underbrace{ \bar{H}}_\text{System} + \underbrace{\bar{H}'}_\text{Wärmebad} + \underbrace{ \bar{H}_i}_\text{Wechselwirkung}
\end{align}
\item System sei im Zustand $r$, $\bar{H} \Psi_r = E_r \Psi_r$ mit Wahrscheinlichkeit $p_r(t)$.

\underline{Master Gleichung}
\begin{align}
\frac{dp_r}{dt}= \sum_s p_s \underbrace{w_{sr}}_\text{} - p_r \underbrace{w_{rs}}_\text{Übergangsraten}
\end{align}
\item Anwendung im Wärmebad
\begin{align}
w_{rs}= & \sum_{r',s'} p_{r'}' w_g(rr' \rightarrow ss') = \frac{1}{Z'} \sum_{r',s'}  e^{-\beta E_{r'}'} w_g (rr' \rightarrow ss') \mbox{ Wärmebad ist kanonisch! } \\
w_{sr}= & \frac{1}{Z'} \sum_{r',s'}  e^{-\beta E_{s'}'} w_g (ss' \rightarrow rr')
\end{align}
\item Energieerhaltung: $E_{r'}' + E_r = E_{s'}' + E_s$ (*)
\item Symmetrie im Gesamtsystem $w_g (rr' \rightarrow ss') = w_g (ss' \rightarrow rr')$(*)
\end{itemize}
aus (*) einsetzen folgt
\begin{align}
w_{sr}&= \frac{1}{Z'} \sum_{r',s'} e^{\beta (E_{r'}' -E_{s'}') } e^{-\beta E_{r'}'  w_g (rr' \rightarrow ss')} = w_{rs} \; e^{-\beta (E_r - E_s)} \\
& \Rightarrow  \frac{w_{sr}}{w_{rs}}= e^{-\beta (E_r - E_s)}
\end{align}
\underline{also:} Irreversible Dynamik eines Systems in Kontakt mit einem Wärmebad wird beschrieben durch Master-Gleichung $ frac{dp_r}{dt}= \sum_s( p_s(t) w_{sr} - p_r(t) w_{rs}) $ mit $\frac{w_{sr}}{w_{rs}}= e^{-\beta (E_r-E_s)}$. Diese Dynamik wird durch Metropolis Algorithmus (oder Heat Bath Algorithmus) simuliert.

\underline{Beachte:} \begin{itemize}
\item $w_{sr}$ liegen nicht absolut fest. Mit $w_{sr}, w_{rs}$ ist auch $\gamma (t) w_{sr}, \gamma(t) w_{rs}$ Lösung \\  $\Rightarrow$ Zeitskala ist nicht absolut festgelegt!
\item $w_{sr}$ ist nicht mikroskopisch bekannt (im Raum nicht festgelegt)
\item Die Master Gleichung beschreibt \textit{ausschließlich} irreversible Dynamiken!
\item speziell für Ising Modell heißt diese Dynamik Glauber-Dynamik.


\end{itemize}
Beispiel: Glauber Dynamik für 2 Spins:  $H=-J S_1 S_2$ mit $S_{1,2} = \pm 1$
$2^2 =4$ Zustände: $ \uparrow_+ \uparrow_+ \; \; \uparrow_+ \downarrow_- \; \; \downarrow_- \uparrow_+ \; \; \downarrow_- \downarrow_-$

 \begin{align*}
 \frac{dp_{++}}{dt} = \sum_s p_s w_{sr} - p_r w_{rs}
 \end{align*}
 (Energie $++$ und $--$ sind genau gleich, da ja kein externes Feld angelegt ist. Nur interessant ist also der Übergang von $++, --$ zu $+-,-+$) $\Rightarrow$ Annahme: Single spin flip und 'Metropolis' mit \begin{align*}
 w=
 \begin{cases}
 1 & \Delta E <0 \\
 e^{- \frac{\Delta E}{kT}} & \Delta E>0
 \end{cases}
 \end{align*}
 Es gibt 
 \begin{itemize}
 \item  $w_{++ \rightarrow --}= w_{+- \rightarrow -+}$ etc...
 \item $w_{+- \rightarrow ++}= w_{-+ \rightarrow ++}=1$ etc...
 \item $w_{-- \rightarrow -+}= w_{-- \rightarrow +-}= w= e^{-\frac{2J}{k_BT}}$ etc... 
 \end{itemize}
 
  \begin{align*}
 \frac{dp_{++}}{dt} =& \sum_s p_s w_{s \rightarrow ++} - p_{++} w_{++ \rightarrow s} \\
 =& p_{+-} w_{+- \rightarrow ++} - p_{++} w_{++ \rightarrow +-} +
 p_{-+} w_{-+ \rightarrow ++} - p_{++} w_{++ \rightarrow -+} =
 p_{+-} + p_{-+} - 2 p_{++} w \\
 \frac{dp_{--}}{dt}= & p_{+-} + p_{-+} - 2p_{--}w \\
  \frac{dp_{+-}}{dt}= & p_{++}w + p_{--} w- 2p_{+-} \\
   \frac{dp_{-+}}{dt}= & p_{++}w + p_{--}w -2p_{-+}
 \end{align*}
$\Rightarrow$ homogenes, lineares Gleichungssystem
$\Rightarrow$ Lösung für $\lambda$ mit 
\begin{align*}
\begin{vmatrix}
-2w+\lambda & 0 & 1 & 1 \\
0 & -2w +\lambda & 1 & 1 \\
w & w & -2w + \lambda & 0 \\
w & w & 0 & -2+\lambda 
\end{vmatrix} \\
\end{align*}
$\Rightarrow \lambda_1=0, \lambda_2=2, \lambda_3=2w+2, \lambda_4=2w$ \\
Wichtig: Kleinstes $\lambda >0$ definiert die Relaxationszeit
$\Rightarrow$ für lange Zeiten: $p_s(t) \approx a_s + b_s e^{-\lambda_4 t} \approx a_s + b_s e^{-\frac{t}{\tau}}$ mit $\tau= \frac{1}{20}= \frac{e^{2J/k_BT}}{2}$ \\
\underline{Magnetisierung:} $M(t)= \sum_s p_s(t) M_s = 2(p_{++}(t) + p_{--}(t) \approx M_0 e^{-\frac{t}{\tau}} + const$, wobei $const=0$. (Formel gilt für $t\to \infty$) \\
\underline{Diskussion:} %TODO image
\begin{itemize}
\item $\tau$ hängt vom 'Algorithmus' ab
\item $p(t)$ hängen von Dynamik ab (single spin flip oder nicht)
\end{itemize}
\subsection{Phasenübergange und Skalentheorie}
\begin{itemize}
\item[•] ideale Gase haben keine Wechselwirkung
\item[•] erst die Wechselwirkung zwischen Teilchen erklärt aber das Entstehen von Ordnung
\item[•] Ordnung entsteht häufig spontan durch PÜ, bei dem eine Symmetrie gebrochen wird.
\item[•]lösbares Modell: Ising-Modell: %TODO image
\end{itemize}
Ordungsparameter; $ lim_{B \to 0} lim_{N \to \infty} M(B)$.\\
Analytisch: 2D, Quadratgitter: \begin{align}
J= - \sum_{i,j} \frac{J}{2}\sigma_i \sigma_j \; , \; \sigma_i=\pm 1
\end{align}
Onsager: \underline{innere Energie:}
\begin{align}
\frac{U}{N}= &- J \; coth( \frac{2J}{k_BT}) \left[ 1 + \frac{2}{\pi} \left( 2 \; tanh^2 \left( \frac{2J}{k_BT} \right) - 1 \right) K_1(\kappa ) \right] \; \mbox{ mit} \\
\kappa=& \frac{2 \; sinh \left( \frac{2J}{k_BT} \right)}{cosh^2 \left( \frac{2J}{k_BT} \right) } \; , \; K_1(\kappa)= \int_0^\frac{\pi}{2} \frac{1}{\sqrt{1- \kappa^2 \; sinh^2(\Phi)}} d\Phi
\end{align}
U ist nicht analytisch bei einer Temperatur $T_c$ mit $sinh(\frac{2J}{k_BT_c} ) = 1 \Rightarrow cosh(\frac{2J}{k_BT_c})= \sqrt{2} \Rightarrow 2 tanh^2( \frac{2J}{k_B T_c}) =1$ mit $k_B T_c = 2.269 J$. %TODO bild
\begin{itemize}
\item[•] spezifische Wärme $c(T \to T_c) \propto - ln \left(\vert 1- \frac{T}{T_c}\vert \right)$
\item[•]Magnetisierung 
\begin{align}
m=
\begin{cases}
0 & T>T_c \\
\sqrt[4]{1+x^2} \sqrt[8]{1-6x^2 +x^4} & T<T_c
\end{cases}
\end{align}
\item[•]PÜ (Phasenübergang) $2.$Ordnung, Ordnungsparameter stetig, kritische Exponenten
\end{itemize}
kritische Exponenten beschreiben Verhalten in der Nähe des kritischen Punktes $(\epsilon = \frac{T - T_c}{T_c} , B) = (0,0)$ \\
spezifische Wärme: $c \propto \epsilon^{-\alpha}$\\
Suszeptibilität: $\chi \propto \epsilon^{-\gamma}$\\
Korrelationslänge: $\xi \propto \epsilon^{-\nu}$\\ 
Magnetisierung (OP) \begin{align}
m= \begin{cases}
M \propto \epsilon^{-\beta} & T<T_c \\
M \propto \vert B \vert ^\frac{1}{\delta} & T=T_c
\end{cases}
\end{align}
genauer: \begin{align*}
A(\epsilon)= 
\underbrace{ A_0 + A_1 \epsilon + A_2 \epsilon +...}_\text{analytisch} 
+ \underbrace{ A_3 \epsilon^{0.8}}_{\substack{ \text{führend} \\ \text{ nichtanalytisch} \\ \text{ (kritisch)} }}
+ \underbrace{ A_4 \epsilon^{1.7}}_\text{Korrekturen}
\end{align*}
\underline{wichtig:} Universalität \\
\underline{Skalenhypothese:} Freie Energie ist eine verallgemeinerte, homogene Funktion: \begin{align}
G(a^{x_1} B, a^{x_2} \epsilon)= a \; G(B, \epsilon)
\end{align} 
für führenden nicht-analytischen Anteil. \\
'normal' wäre: $U( \lambda S, \lambda V, \lambda N) = \lambda U(S,V,N))$ \\
Beispiel: $f(x) = x^{0,5} \Rightarrow f(bx) = (bx)^{0,5} = b^{0,5} x^{0,5}$ \\ \begin{align*}
b=a^2 \;  \Aboxed{ f(a^2 x)=  a x^{0,5} = a f(x)}
\end{align*}
'normal' f analytisch $\Rightarrow f(x)= f(0) + f'(0)x + .... \Rightarrow \Delta f(x) = f(x) - f(0) = f'(0)x+...$
\begin{align}
\Rightarrow \Aboxed{ \Delta f(ax) = f'(0) ax= a \Delta f(x)}
\end{align}
\underline{Folgerungen:} setze $ a= \vert \epsilon \vert ^{-1/x^2}$
\begin{align}
\Rightarrow G\left(\vert \epsilon \vert ^{-\frac{x_1}{x_2}} B, \underbrace{ \frac{\epsilon}{\vert \epsilon \vert}}_{\pm 1} \right) = \vert \epsilon \vert^\frac{-1}{x^2} G(B, \epsilon)
\end{align}
\underline{Definition:} Skalenfunktion: \begin{align}
f^\pm \left( \frac{B}{\vert \epsilon \vert ^\frac{x_1}{x_2}} \right) = G \left( \epsilon^\frac{-x_1}{x_2} B, \pm 1 \right)
\end{align} 
\underline{Skalenhypothese:}
\begin{align}
G(B,\epsilon) = \vert \epsilon \vert ^\frac{1}{x_2} \; f^\pm \left( \frac{B}{\vert \epsilon \vert ^\frac{x_1}{x_2}} \right) \\
G(B,\epsilon) = \vert B \vert ^\frac{1}{x_1} \; g \left( \frac{\epsilon}{\vert B \vert ^\frac{x_2}{x_1}} \right)
\end{align}
\begin{itemize}
\item[•]Eigenschaften der Skalarfunktionen $g,f$: G soll am kritischen Punkt endlich bleiben $\Rightarrow f^\pm (0)$ und $g(0)$ bleiben endlich (und ihre Ableitungen) $\Rightarrow \alpha, \beta, \gamma, \delta$ lassen sich durch $x_1, x_2$ ausdrücken: \\
\underline{spezifische Wärme:} \begin{align}
G(0, \epsilon)= \vert \epsilon \vert
^\frac{1}{x_2} f^\pm (0) \; , \; B=0 \\
\Rightarrow c \propto \frac{\partial^2 G}{\partial \epsilon^2} \propto \vert \epsilon \vert ^{\frac{1}{x_2}-2}= \vert \epsilon \vert ^{-\alpha} \; , \; \Rightarrow \alpha = 2- \frac{1}{x_2} 
\end{align}
\underline{spontane Magnetisierung:} 
\begin{align}
M(0,\epsilon) \propto \frac{\partial G(B, \epsilon)}{\partial B} \vert \epsilon \vert ^\frac{1}{x_2} f^\pm \left(\frac{B}{\vert \epsilon \vert ^\frac{x_1}{x_2}} \right) \propto \vert \epsilon \vert ^\frac{1}{x_2} \frac{1}{\vert \epsilon \vert ^\frac{x_1}{x_2}} \propto \vert \epsilon \vert ^\beta \Rightarrow \beta = \frac{1-x_1}{x_2}
\end{align}
\underline{Suszeptibilität:}
\begin{align}
\xi = \frac{\partial M}{\partial B} \Bigg\vert_{B \to o}
 \propto \vert \epsilon \vert ^\frac{1}{x_2} \frac{1}{\vert \epsilon \vert ^\frac{2 x_1}{x_2}} \Rightarrow \gamma = \frac{2x_1 -1}{x_2}
\end{align}
\underline{Magnetisierung für $\epsilon =0$:}
\begin{align}
M(B,\epsilon) = - \frac{\partial G}{\partial B} = - \frac{\partial}{\partial B} \left[ \vert B \vert ^\frac{1}{x_1} g\left( \frac{\epsilon}{\vert B \vert ^\frac{x_2}{x_1}} \right) \right] \\
= - B^{\frac{1}{x_1}-1} g\left( \frac{\epsilon}{B^\frac{x_2}{x_1}}\right) - \vert B \vert ^\frac{1}{x_1} \frac{x_2}{x_1} \frac{\epsilon}{\vert B \vert ^{\frac{x_2}{x_1}+1} } g'\left( \frac{\epsilon}{\vert B \vert ^\frac{x_2}{x_1} }\right) \\
= - B^{\frac{1}{x_1}-1} \left[ 
g\left( \frac{\epsilon}{B^\frac{x_2}{x_1}}\right) 
- \frac{x_2}{x_1} \frac{\epsilon}{\vert B \vert ^{\frac{x_2}{x_1}} } g'\left( \frac{\epsilon}{\vert B \vert ^\frac{x_2}{x_1} }\right) \right] \\
\Rightarrow \Aboxed{M(B,\epsilon)= B^\frac{1}{\delta} \text{\~{g}} \left( \frac{\epsilon}{B^\frac{1}{\beta \delta}} \right)  }\\
\frac{M(B,\epsilon)}{\underbrace{B^\frac{1}{\delta}}_\text{Grösse hängt von einem Feld ab}}=  \text{\~{g}} \left( \frac{\epsilon}{\underbrace{ B^\frac{1}{\beta \delta}}_{ \substack{ \text{hängt von} \\ \text{zwei Feldern} \\ \text{ ab}}}} \right) 
\end{align} %TODO formel schöner
\underline{vorher:} $M = M(B,\epsilon)$ \\ %TODO image
\underline{Crossover-Verhalten} \\
Skalarverhalten: \begin{align}
G(B,\epsilon)=& \vert \epsilon \vert ^\frac{1}{x_2} f^\pm \left( \frac{B}{\vert \epsilon \vert ^\frac{x_1}{x_2}} \right) \\
G(B,\epsilon)= & \vert B \vert ^\frac{1}{x_1} g \left( \frac{\epsilon}{B ^\frac{x_1}{x_2}} \right) \; , \; B>0 \\
\Rightarrow & B=0: \; G(0,\epsilon) \propto \vert \epsilon \vert ^\frac{1}{x_2} \propto \vert \epsilon \vert^{2- \alpha} \\
\Rightarrow & B \neq 0: \mbox{ solange } \frac{B}{\vert \epsilon \vert ^\frac{x_2}{x_1}} << 1 \Leftrightarrow B< \vert \epsilon \vert ^\frac{x_1}{x_2} \\
& \mbox{ gleiches kritisches Verhalten wie bei } B=0. \frac{B}{\vert \epsilon \vert^\frac{x_1}{x_2}} \approx 1 
\end{align} ändert sich das Verthalten. $\to$ 'Crossover' zu anderem (oder keinem) kritischen Verhalten. \\
crossover für $B \approx \vert \epsilon \vert^\frac{x_1}{x_2} = \vert \epsilon \vert ^\Phi$
\subsection{Finite size scaling}
betrachten $\frac{1}{L}$ als Skalenfeld.  \\
Ansatz: $G(B,\epsilon, \frac{1}{L}) = \vert \epsilon \vert^{2-\alpha} f \left( \frac{\frac{1}{L}}{\vert \epsilon \vert^\Phi}\right) \; , \; \Phi = \mbox{ Crossover }$
erwarten, dass $G$ nur von dimensionaslosten Größen abhöngt:
\begin{align}
\frac{•}{•}
\end{align} %TODO %TODO inhalt fehlt
\item[•] $L >> \xi (T) \Rightarrow$ wie unendliches System
\item[•] $ L \approx \xi (T)$ crossover $\vert \epsilon \vert \propto L^\frac{-1}{\nu}$
\item[•]$L<< \xi(T)$ Singularitäten verschwinden
\end{itemize}
\underline{Beispiel:} $B=0$, spezifische Wärme: $c\propto \frac{\partial^2 G}{\partial \epsilon^2} \propto \vert \epsilon \vert^{-\alpha} f^\pm \left( \frac{1}{L \vert \epsilon \vert^\nu}\right) \propto L^\frac{\alpha}{\nu} (L^\frac{1}{\nu} \vert \epsilon \vert)^{-\alpha} f^\pm() \\
\Rightarrow \frac{c}{L^\frac{\alpha}{\nu}} = \text{\~{f}} (L^\frac{1}{\nu} \epsilon)$ und $c \propto L^\frac{\alpha}{\nu}$ bei $T_c$.\\
\underline{ohne Beweis:}$ L^\frac{\beta}{\nu} M(\epsilon,L)=f(L^\frac{1}{\nu} \epsilon)$

 
zu jedem $\vec{S}$ mit $M(\vec{S})$ gibt es einen (bei$ B=0$) gleichwahrscheinlichen Zustand $\vec{S}'= -\vec{S}$ mit $M(S') = - M(\vec{S})$. \\
$\Rightarrow$ es ist $\langle M(T,B=0)  \rangle \geq 0$ (Statistisches Mittel $\approx$ MC-Mittel \\
\underline{Symmetriebruch} nur für: $ \langle M(T,B=0) \rangle = lim_{B \to 0} lim_{N \to \infty} \langle M(T,B) \rangle $
auch experimentell beobachtbar: endliches System fluktuiert thermisch $(\pm M),$ sodass im zeitmittel $M \to 0$
\\
\underline{Superparamagnetismus} \\
\underline{Zeitskala:} $\tau = \tau_0 e^\frac{\Delta E}{k_B T}$ \\
\underline{Energieverbrauch:} $\Delta E(L^n)$ (n meint Dimension: Oberfläche, Volumen...)\\ %TODO spinbild
$\Rightarrow$ zeitabhängiger (Algorithmus-abhängige) Ordungsparameterkurven. \\
\underline{besser:} $M(T,B=0) = \sqrt{\langle \left( \frac{1}{N} \sum_{i=1}^N S_i \right)^2 \rangle} = \sqrt{\langle S^2 \rangle}$ \\
hiermit wird finite-size Analysis durchgeführt:
\begin{itemize}
\item[•] innere Energie: $U= \langle H \rangle$
\item[•] spezifische Wärme: $c_v= \frac{1}{k_B T} \left( \langle H ^2 \rangle - \langle H \rangle^2 \right) $
\item[•] Binder-Kummulante (4$^{th}$ order cummulant) zur Bestimmung von $T_c: U_L = 1-  \frac{\langle S^4 \rangle}{3 \langle S^2 \rangle^2}$ mit folgenden Eigenschaften:
\begin{enumerate}
\item $T > T_c , L \gg \xi: \; U_L \to 0 \approx L^{- \alpha}$
\item $ T< T_c, L \gg \xi: \; U_L \to \frac{2}{3}$
\item $L \ll \xi: U_L \to U^*, L, \epsilon$ unabhängiger Zahlenwert
\end{enumerate}

\end{itemize}

 \subsection{Kontinuierliche Freiheitsgrade: 'x-y' und Heisenbergs-Modell} \label{Kapitel2.9}
 Heisenberg-Ferromagnet mit axialer Anisotropie: 
 \begin{align}
 H= - J \sum_{i,j} \vec{S}_i \vec{S}_j - K \sum_i S_{iz}^2 \; , \; \vert \vec{S} \vert
 =1
 \end{align}
 \underline{Universalität:} Kriterium hängt von Dimensionen des Ordnungsparameters ab:
 \begin{enumerate}
 \item K= 0: $S_i \cdot S_j$ isotrop $\Rightarrow$ OP hat $n=3$ Komponenten $(m_x, m_y, m_z)$
 \item $K<0$ Spins legen sich in $x-y-$Ebene, OP hat $2$ Komponenten
 \item $K>0$ Spins entlang $z-$Achse, OP hat 1 Komponente
 \end{enumerate}
 \begin{itemize}
 \item entlang der $K=0$-Linie anderes kritisches Verahlten (andere Exponenten) als für $K=0$.
 \item für jedes endliche $K$ ist man im Universalitätsklasse von Ising- oder 'x-y'-Modell. 
 \item $K=0$: Heisenberg
 \end{itemize}

%TODO %TODO %TODO Fehleintrag nachholen!

\begin{enumerate}
\item $x-y-$Modell: 1 Spin

\begin{itemize}
\item ergodisch
\item symmetrisch
\end{itemize}

\item Heisenberg-Modell
\begin{align}
H= -\frac{J}{2} \sum_{i,j} \vec{S}_i \vec{S}_j - D \sum_i S_{iz}^2 - \vec{B} \sum_i \vec{S}_i\; , \; \vert \vec{S}_i \vert =1
\end{align}
single spin flip algorithm: Phasenraum des einzelnen Spins ist Einheitskugeloberfläche %TODO image

\begin{enumerate}
		\item gleichverteilt auf Einheitskugel
			\begin{itemize}
				\item \underline{rejection Methode:}
					\begin{itemize}
						\item  ziehe 3 Zufallszahlen $\in [ -1:1]$. $z_x, 		z_y, z_z$. 
						\item verwerfe die Wahl, wenn außerhalb der Einheitskugel. $\sqrt{z_x^2 + z_y^2 +z_z^2}>1$.
						\item sonst normieren: 
							\begin{align}
							TODO
							\end{align}
					\item schnelle Methode, beruht auf 3 ZZ + Algebra. Relaive Zahl der berücksichtigen Tripel ist $ \frac{4}{3} \pi / 2^3 = \frac{\pi}{6}$
					\end{itemize}				
			\item \underline{Kugelkoordinaten} Flächenelement:
			\begin{align}
			dF= \underbrace{r^2}_\text{=1} sin(\Theta) d \Theta d\phi
			\end{align}	
			Würdem man gleichverteilte ZZ in $\Theta$ und $\phi$ nehmen, wären die Vektoren auf der Einehitskugeloberfläche nicht gleichverteilt, da $dF \propto sin(\Theta)$ \\
			\underline{Aber:} $sin(\Theta) d\Theta = - d cos(\Theta) = -dz$ mit $S_z =cos(\Theta)$. \\
			$\Rightarrow dF= - d S_z d \phi \; \Rightarrow$ man kann $S_z$ und $\phi$ gleichverteilt aus $[-1:1]$ und $[0:2\pi)$ wählen. $\Rightarrow$ %TODO vector
			\\ \underline{Nachteil:} $cos(),sin()$ müssen gerechnet werden $\to$ langsam!			
			\end{itemize}
			
			\item kleine Schritte um den alten Vektor $\vec{S}$ innerhalb eines Kegels
			\begin{itemize}
			\item erzeuge Zufallsvektor $\vec{S}_r$ mit maximalen Radius R.
			\item rechne 
			\begin{align}
			\vec{S}_r = \frac{\vec{S} + \vec{S}_r}{\sqrt{\vec{S}^2 + \vec{S}_r^2}}
			\end{align}
			\item $\vec{S}_r$ aus rejection Methode ohne Normierung
			\\
			\\
			
			
Diese Methode erzeugt kleine Schritte, sodass Energiebarrieren überwunden werden müssen $\to$ Konsequenz für die Dynamik. Man kann maximale Schrittweite R berechnen um einen Monte Carlo Schritt zu eichen $ßto$ Zeitquantifizierung ('time quantified Monte Carlo')
			\end{itemize}
\end{enumerate}
\end{enumerate}
 \subsection{Perkolation}
 	\begin{itemize}
 	\item betrachte Gitter in d Dimensionen
 	\item besetze Plätze (Verbindungen) mit Wahrscheinlichkeit p
 	\item definiere Nachbarschaft, z.B via nächste Nachbarn
 	(je nach fall vielleicht auch übernächste nachbarn bei dottierung einer Monolage beispielsweise...)
 	\item Perkolation beschäftigt sich mit Clustern, d.h.  besetzte, benachbarte Gitterplätze
 	\item oberhalb der sogenannten Perkolationswahrscheinlichkeit (oder Perkolationsgrenze) $p_c$ gibt es einen $\infty$ großen, perkolierenden Cluster. 
 	$\Rightarrow$ wichtig für Leitfähigkeit, magnetische Ordnung
 	\begin{tabular}{|c|c|c|c|}
 	\hline 
 	 &  & $p_c$ & bond \\ 
 	\hline 
 	\textbf{2d} & Quadrat & 0.592 & 0.5 \\ 
 	\hline 
 	• & sc & 0.312 & 0.249 \\ 
 	\hline 
 	\textbf{3d} & fcc & 0.198 & 0.119 \\ 
 	\hline 
 	• & bcc & 0.245 & 0.179 \\ 
 	\hline 
 	\end{tabular} 
 	\end{itemize}
 	\underline{Definiere Korrelationsfunktion $g( \vert \vec{r}_1 - \vec{r}_2 \vert)$:} Wahrscheinlichkeit, dass $\vec{r}_1$ und $\vec{r}_2$ zum gleichen Cluster gehören. \\
 	\underline{Korrelationslänge:}
 	 \begin{align}
 	\xi^2= \frac{\sum_r r^2 g(r)}{\sum_r g(r)}
 	\end{align}
 	Definition ist vereinbar mit $g(r) \propto e^\frac{-r}{\xi}$, da 
 	\begin{align}
 	\frac{\int d^3r r^2 \; e^\frac{-r}{\xi}}{\int d^3 r \; e^\frac{-r}{\xi}}
 	= \frac{\int_0^\infty dr r^4 \; e^\frac{-r}{\xi}}{\int_0^\infty d r \; r^2 \; e^\frac{-r}{\xi}}
 	= \frac{\xi^s}{\xi^3}  \frac{\int d^3r r^2 \; e^\frac{-r}{\xi}}{\int d^3 r \; e^\frac{-r}{\xi}}
 	\end{align}
 	\begin{itemize}
 	\item $p>p_c$: Beitrag des $\infty$-großen Clustern wird subtrahiert $\Rightarrow \; g(r)$ geht immer gegen Null
 	\item $p \to p_c:$ $\xi(p) \propto \vert p-p_c \vert^{-\nu}$ mit kritischem Exponenten $\nu$.
 	
 	\end{itemize}

 	\underline{Universalität:} $\nu_{2d}= \frac{4}{3}, \; \nu_{3d}=0.88$
 	
 	 	Wir betrachten die Wahrscheinlichkeitsvertielung $D(s,p)$ für Cluster der Größe $s$: \begin{align}
 	D(s,p)=& (1-p)^2 \; p^s \\
 	=& (1-p)^2 \; e^{ln(p)s} = (1-p)^2 e^{- \vert ln(p) \vert s} = (1-p)^2 e^{-\frac{s}{\xi}}
 	\end{align}
 	mit Korrelationslänge $\xi = \frac{1}{\vert ln(p) \vert} = -\frac{1}{ln(p)}$ und mit $ln(p)= p-1= p-p_c$ wobei $p_c=1$. 
 	
 	$\Rightarrow \xi (p \to p_c) = \frac{1}{p_c-p}= (p_c - p)^\nu $ mit dem kritischen Exponenten $\nu =1$ \\
 	
 	In hpheren Dimensionen $D>1$ nur numerische Verfahren möglich. nummerisches Verfahren zur Clusteranalyse.
 		\begin{enumerate}
 		
 		\item Rekursiv %TODO image
 			\begin{itemize}
 			\item Schleife durch das Gitter
 			\item 1.Besetzter Platz erhält Index 1
 			\item Nachbarn werden besucht, falls besetzt nicht indiziert
 			\item weiter zum nächsten Platz falls besetzt und falls nicht indiziert, dann Index 2...
 			\end{itemize}
 			
 		\item schneller: Hoshen-Kopelmann-Algorithmus
 		
\end{enumerate} 	

\underline{Cluster-Analyse} erlaubt weitere Auswirkung der Verteilung $D(s,p)$.
\begin{itemize}
\item in der Nähe des kritischen Punktes $p_c$ gilt $D(s,p_c) \propto s^{-\epsilon}$ mit einem kritischen Exponenten $\epsilon$.
\item weiter weg gilt $D(s,p) \propto e^{-\frac{s}{\xi}}$ mit $\xi (p) \propto \vert p_c - p \vert ^{- \nu}$
\item das perkulierende Cluster am kritischen Punkt ist ein Fraktal
\end{itemize}	 
\underline{Fraktal:} \begin{itemize}
	\item gebrochene Dimension \begin{align}
	s \propto \underbrace{r^{D_f}}_\text{Gyrationsradius} \mbox{ fraktale Dimension } D_f
\end{align}	 
\item 'normal' wäre Fläche $F \propto r^2$ und Volumen $V \propto r^3$.
\item $D_f$ ist dabei kleiner als die einbettende Dimension $(2,3)$
\item Fraktale sind selbstähnlich
\end{itemize}
Beispiel: aus der Mathematik %TODO image fraktale

allgemein: $ln(as)= a^x ln(x)$ mit $4=3^x \Rightarrow x= \frac{lln(4)}{ln(3)}$ \\
setze: $a= \frac{1}{s} \Rightarrow ln(1)= \left( \frac{1}{s}) \right)^x ln(s) \Rightarrow ln(s)= ln(1) s^x$

\section{Molekulardynamiksimulationen}
Es gibt 2 Klassen von Verfahren in der statistischen Physik 
\begin{enumerate}
\item Monte Carlo (kanonische Gesamtheit)
\item Molekulardynamik (zunächst mikrokanonisch)
Lösen Bewegungsgleichungen für viele Teilchen \\
$\Rightarrow$ brauchen Verfahren zur nummerischen Lösung von DGL \\
\underline{Dynamische Systeme} ist beispeilsweise die Newtonsche Bewegungsgleichung \begin{align}
m \ddot{\textbf{r}} = \mathbf{F} (\mathbf{r}, \dot{\mathbf{r}}, t)
\end{align} mit der Bahn $\mathbf{r}(t)$.
neue Variablen sind \begin{align*}
\dot{\mathbf{r}}= \mathbf{v} \\
\dot{\mathbf{v}} = \frac{1}{m} \mathbf{F}(\mathbf{r}, \mathbf{v},t) 
\end{align*}
sodass 
\begin{align*}
\begin{pmatrix}\mathbf{r}\\ \mathbf{v}\end{pmatrix} \in \mathbb{R}^6
\end{align*}
Es handelt sich um 6 DGLn 1.Ordnung (siehe auch 'Hamilton Formalismus'). Es genügt eine DGL folgenden Typs zu studieren: 
\begin{align}
\dot{\mathbf{x}}= \mathbf{f}( \mathbf{x},t) \mbox{  mit  } x \in \mathbb{R}^n \mbox{   (Dynamisches System)}
\end{align}
\subsection{Euler Verfahren}
Eindimensional $\dot{x}= f(x,t)$.\\  %TODO image 
\underline{Taylor:}
\begin{align}
x(t_0 +h)=& x(t_0) + \dot{x}(t_0)h+ \mathcal{O} (h^2) \\
=& x(t_0) + f(x,t_0) + \mathcal{O} (h^2)
\end{align}
\underline{Diskretisierung der Zeit:}
\begin{align}
t_n = t_0 + nh, \; n=0,1,2,...
\end{align}
\underline{Euler:}
\begin{align}
\Aboxed{ x_{n+1}= x_n + f(x_n, t_n)h+ \mathbb{O}(h^2)}
\end{align}
Fehler des Einzelschritts $\propto h^2$. Für ein Intervall der Länge $T$ benötigt man $N= \frac{T}{h}$ Schritte.
\begin{align}
\Aboxed{ \mbox{Gesamtfehler} \propto h^2 N \propto h }
\end{align}
Fehler oben heißt systematischer Fehler, er entsteht durch die Approximation. Verkleinern durch $h \to 0$. Geht das? Nein! Wegen \underline{Rundungsfehlern} und eventueller \underline{Instabilitäten}.
\end{enumerate}

\subsection{Stabilitätsanalyse}
$x_n$: berechne Werte $x(t)$ mit Fehler $\epsilon_n$:
\begin{align}
\mathbf{x}_{n+1} + \mathbf{\epsilon}_{n+1} =\mathbf{x}_n + \mathbf{\epsilon}_n + f( \mathbf{x}_n + \mathbf{\epsilon}_n, t_n) \Delta t := \mathbf{T}(\mathbf{x}_n + \mathbf{\epsilon}_n)
\end{align} %TODO %TODO griechische buchstaben in boldface?!?
 entwickle $T$ für kleines $\epsilon$: 
 \begin{align}
 \mathbf{T}(\mathbf{x}_n + \mathbf{\epsilon}_n) 
 \approx \mathbf{T}(\mathbf{x}_n) + \underbrace{\frac{d\mathbf{T}}{d\mathbf{x}}}_\text{Funktionalmatrix (Jacobi)} \cdot \mathbf{\epsilon}_n \\
 \Rightarrow \epsilon_{n+1} = \frac{d\mathbf{T}}{d\mathbf{x}} \vert _{x_n} \epsilon \equiv \mathbb{G} \mathbf{\epsilon} \; ,
 \end{align}
 
 %TODO interrupt
....wenn für alle Eigenwerte von $\mathbb{G}$, $\vert g_i \vert <1$ gilt:
\\
Beispiel 1: \begin{align}
\frac{dx}{dt}= - \lambda x \mbox{  (zum Beispiel radioaktiver Zerfall für } \lambda<0)
 \end{align}
 \underline{Euler:}
 \begin{align}
 x_{n+1} =& x_n ( 1- \lambda \Delta t) = T (x_n) \\
 \Rightarrow &\epsilon_{n+1} = \frac{dT}{dx} \vert _{x_n} \epsilon_n = (1- \lambda \Delta t) \epsilon_n \\
 \Rightarrow & \vert 1 - \lambda \Delta t \vert <1 , \mbox{  für alle } \lambda < 0 \\
 \Rightarrow & \mbox{stabil!}
 \end{align}
(instabil für $\lambda < 0$) \\
Beispiel 2: \begin{align}
\ddot{z}= - \omega^2 z & \mbox{  (harmonischer Oszillator)} \\
\begin{cases}
\dot{x}_1 = x_2 \\
\dot{x}_2 = - \omega^2 x_1 
\end{cases} 
\Rightarrow &
\begin{cases}
x_1 (n+1) = x_1(n) + x_2 (n) \Delta t \\
x_2 (n+1) = x_2(n) - \omega^2 x_1 (n) \Delta t
\end{cases}
\Rightarrow \mathbb{G} = 
\begin{pmatrix}
 1 & \Delta t \\
 - \omega^2 \Delta t & 1
\end{pmatrix}
\end{align}
für Eigenwerte aus 
\begin{align}
\begin{vmatrix}
1-g & \Delta t \\
- \omega^2 \Delta t & 1-g
\end{vmatrix}
= 0 = (1-g)^2 + \omega^2 \Delta t^2 \\
\Rightarrow g_{1,2} = 1 \pm i\omega t \\
\Rightarrow \vert g_{1,2} \vert = \sqrt{1 + \omega^2 \Delta t^2} <1\; \forall \omega, \Delta t \\
\Rightarrow \mbox{ immer instabil}
\end{align}
Anschaulich: \begin{align}
\dot{x}= f(x) = - \lambda x \; \Rightarrow x(t) = x_0 e^{-\lambda t}
\end{align}
\underline{Weitere Tests (nummerisch):}\\
\begin{itemize}
\item Energieerhaltung (oder Impuls, Drehimpuls) 
\item Rückwärtsintegration
\subsection{Runge-Kutta Verfahren}
besser: stabilder; schnellere Konvergenz \\
Warum schneller? $\to$ CPU-Zeit $\Leftrightarrow$ bessere Konvergenz.\\

\underline{Euler:} 

\begin{align}
x(t_n +h) =& x(t_n) + \dot{x}(t_n) \cdot h + \mathcal{O}(h^2) \\
x(t_n -h) =& x(t_n) - \dot{x}(t_n) \cdot h + \mathcal{O}(h^2)  \\
 \Rightarrow x(t_n +h) - x(t_n -h) =&2 h \dot{x}(t_n) \cdot  + \mathbb{O}(h^3)
\end{align}
Lösung von $\dot{x}= f(x,t):$
\begin{align}
x_{n+1}= x_{n-1} + 2 h f(x_n, t) + \mathcal{O} (h^3) \\
x_{n+2}= x_{n} + 2 h f(x_{n+1}, t) + \mathcal{O} (h^3)
\end{align}
und nennt dies Bocksprung, Leop-Frog, Runge Kutta 1.Stufe...\\
\underline{noch besser:} Runge-Kutta 2.Stufe:
\begin{align}
x_{n+1} =& x_{n-1}+2hf(x_{n}, t_n) + \mathcal{O}(h^3) \\
\mbox{ berechne } x_n \mbox{ aus Euler: } x_n = &x_{n-1} + h f(x_{n-1}, t_{n-1}) + \mathcal{O}(h^2) \\
\mbox{ umschreiben: } 2h=&\bar{h} : 
\end{align}
\begin{align}
\Aboxed{K=\frac{1}{2} \bar{h} f(x_n, t_n) } \\
\Aboxed{x_{n+1}= x_n + \bar{h} f(x_n+k, t_n+\frac{1}{2}\bar{h}) + \mathcal{O}(h^3) } \\
\Aboxed{t_{n+1} = t_n + \bar{h} } \\
\mbox{ Runge-Kutta 2.Stufe, Zwischenschrittverfahren }
\end{align}
\underline{Bsp:}\begin{align}
\ddot{x}= f(x,t) \Rightarrow \begin{cases}
\dot{x}=v \\
\dot{v}= f(x,v,t)
\end{cases}
\end{align}
\underline{RKZ:}\begin{align}
K_x =& \frac{1}{2} h v_n \\
K_v = & \frac{1}{2} h f(x_n, t) \\
x_{n+1} =& x_n + (v_n + K_v) h \\
v_{n+1}= & v_n + f(x_n+K_x, (v_n+K_v), t_n+\frac{1}{2}h)h
\end{align}
\underline{(ohne Beweis:) Runge Kutta 4.Stufe:}
\begin{align}
K_1= hf(x_n, t_n) \\
K_2=h f(x_n + \frac{1}{2} K_1, t_n + \frac{h}{2}) \\
K_3= h f(x_n + \frac{1}{2} K_2, t_n + \frac{h}{2}) \\
K_4= h f(x_n +  K_3, t_n + h) \\
x_{n+1}= x_n + \frac{1}{6} (K_1 + 2 K_2 + 2 K_3 + K_4) \\
t_{n+t} = t_n + h + \mathcal{O}(h^5)
\end{align}
Prinzip:
\begin{align}
\dot{x}=& f(x,t) \\
x_{n+1} - x_n =& \int_{t_n}^{t_n+h} dt f(x(t), t) \mbox{  allerdings ist } x(t) \mbox{ nicht bekannt}
\end{align}
\end{itemize}
\subsection{Schrittweitenanpassung}
\begin{itemize}
\item einfachste Vorgehensweise: feste Schrittweite $h +$Tests.
\item unter Umständen ist aber eine Anpassung der Schrittweite besser. Anpassung durch Vergleich (z.B. von RKZ und RK4) \\
RK2: \begin{align}
x_{n+1} = x_n + K_2 + \mathcal{O}(h^3)
\end{align}
RK4: \begin{align}
x_{n+1}= x_n + \frac{1}{6} (K_1 + 2K_2 +2K_3 + K_4) = x_n + K_2 + \underbrace{\frac{1}{6} (K_1 - 4K_2 + 2K_3 +K_4)}_\text{$\delta$} + \mathcal{O}(h^5)
\end{align}
$\delta$ ist ein Maß für den Fehler von RKZ und $\delta= \mathcal{O}(h^3)$. Definiere den relativen Fehler: 
\begin{align}
\left| \frac{\delta}{K_2} \right| = \mathcal{O}(h^2)
\end{align}
und versuchen diesen Fehler konstant zu halten.
\begin{align}
\left| \frac{\delta}{K_2} \right| = a h^2 \overset{!}{=} \epsilon
\end{align}
wird vorgegeben durch Wahl von $h$. Verlange 
\begin{align}
a h_{neu}= \epsilon \Rightarrow h^2 = \frac{\epsilon}{a} = h^2 \frac{\epsilon}{\left| \frac{\delta}{K_2} \right|} \\
\Rightarrow \Aboxed{ h_{neu} = h \sqrt{\frac{\epsilon}{\vert \frac{\delta}{K_2} \vert }}}
\end{align}
\underline{also:}
\item $h$ vorgegeben \begin{align}
\to \epsilon = \left| \frac{delta}{K_2} \right|
\end{align}
\item immer wieder $h$ neu berechnen aus $\vert \frac{ \delta}{K_2} \vert$
\end{itemize}

GROSSER SCHNITT

\subsection{whatever}
\subsection{Chaotische Systeme und fraktale Dimensionen}
\underline{Beispiel:} (Diffusion limited aggregation) DLA
\begin{itemize}
\item Simulation, z.B. d=2, Quadratgitter

\end{itemize} 
\subsection{Molekulardynamik im mikrokanonischen Ensemble}
 moderne Physik: Vielteilchensysteme
 \underline{speziell:} klassische Teilchen (z.Bsp. Moleküle...daher auch die Namensgebung)
 $\to$ können durch Bewegungsgleichungen beschrieben werden \\
 \begin{itemize}
 \item (Intensive Wechselwirkungen ) $\hat{=} $MD-Simulationen 
 \item Teilchen können innere Freiheitsgrade haben. Wird im einfachsten Fall vernachlässigt $\to $ Kugel mit Schwerpunktskoordinate $\vec{r}_i$:
 
 \item $N$ Teilchen, die wechselwirken mit Potential $V(\vec{r}_1,\vec{r}_2, ..., \vec{r}_N)$ 
 
 \begin{align}
 \Rightarrow \; \mbox{Newton } \Aboxed{ m_i \vec{\ddot{r}}_i = -grad_{\vec{r}_i} V(\vec{r}_i, ..., \vec{r}_N)} \; i=1,...,N \\
 \mbox{Anfangsbedingungen } \vec{r}_i^{(0)}, \dot{\vec{r}}_i^{(0)}
 \end{align}
 \item ohne Dissipation erhalten diese MD Simulationen die Energie und Teilchenzahl $\Rightarrow$ \underline{mikrokanonisches Ensemble} innere Energie U durch Anfangsbed. vorgegeben
 \item Entropie $S(U,V,N)$ wird maximiert
 
 \end{itemize}
 \underline{Zwei Vorgehen:}
 \begin{enumerate}
 \item Teilchen dicht gepackt, sie wechselwirken zu jeder Zeit $\Rightarrow$ diskretisieren Zeit $t \to \Delta t$, lösen DGL zu jedem Zeitschritt (\textbf{Zeitgesteuerte MD Simulation}), zu englisch '\textit{time step driven}'.
 \item Teilchen weit auseinander (im Verhältnis zur Reichweite der Wechselwirkung) $\to$ die Teilchen fliegen frei auf endlichen Strecken $\to$ zwischen Stößen wird die DGL analytisch gelöst nur für den Stoßprozess wird DGL nummerisch gelöst \\
 $\Rightarrow$ \underline{Ereignisgesteuerte Simulation} ('\textit{event driven}')
 
 \end{enumerate}
Zu 1) TD: populär ist der Verlet Algorithmus, den es in 2 Versionen gibt:
\begin{align}
\vec{r}_i (t+h) =& r_i(t) + h \vec{\dot{r}}_i(t) + \frac{h^2}{2} \vec{\ddot{r}}_i(t) + \mathcal{O} (h^2) \\
\Rightarrow \vec{r}_i (t-h) =& r_i (t) - h \vec{\dot{r}}_i (t) + \frac{h^2}{2} \vec{\ddot{r}}_i (t) - \mathcal{O} (h^2) \\
\Rightarrow 
 \Aboxed{\vec{r}_i (t+h) =& 2 r_i (t) - \vec{r}_i (t-h) + h^2 \vec{\ddot{r}}_i (t) +\mathcal{O} (h^2)} \\
 \mbox{mit } \vec{\ddot{r}}_i (t)=& \frac{\vec{F}(\vec{r}_1 (t),...., \vec{r}_N(t))}{m} 
\end{align} 
 
Anfangsbedingungen $ \vec{r}_i (0), \vec{\dot{r}}_i (0) \overbrace{\Rightarrow}^\text{Euler} \vec{r}_i (h) = \vec{v}_i (0) h + \vec{r}_i (0)$
Dabei wird $\vec{r}_i (t+h)$ wird berechnet, ohne dass $\vec{v}_i$ berechnet wird

%TODO MISSING Line!

zu 2) Geschwindigkeitsvariation: (häufiger)
\begin{align}
\vec{r}(t+h) = \vec{r}(t) + \vec{\dot{r}} (t) h + \frac{h^2}{2} \vec{\ddot{r}}(t) + \mathcal{O}(h^3) \\
\vec{\dot{r}}(t+h) = \vec{\dot{r}}(t) + \vec{\ddot{r}} (t) h + \frac{h^2}{2} \vec{\dddot{r}}(t) + \mathcal{O}(h^3) 
\end{align}
und
\begin{align}
h \vec{\dddot{r}}(t) = \vec{\ddot{r}} (t+h) - \vec{\ddot{r}} (t) + \mathcal{O}(h^2) \\
\Rightarrow \Aboxed{ \vec{\dot{r}} (t+h) = \vec{\dot{r}} (t) + \frac{h}{2} ( \vec{\ddot{r}} (t) + \vec{\ddot{r}} (t+h) ) + \mathcal{O}(h^3) }  
\end{align}
mit
\begin{align}
\vec{\ddot{r}} (t)= \frac{\vec{F} (\vec{\dot{r}}_1 (t), ... , \vec{\dot{r}}_N (t) ) }{m} \; , \; 
\vec{\ddot{r}} (t+h)= \frac{\vec{F} (\vec{\dot{r}}_1 (t+h), ... , \vec{\dot{r}}_N (t+h) ) }{m}
\end{align}
Anfangsbedingungen $\vec{r}_i (0), \vec{\dot{r}}_i (0), \Rightarrow \vec{F}(\vec{r}_1 (0), ..., \vec{r}_N (0))$
\begin{align}
\mbox{1. Schritt: } & \vec{r}(t+h) \mbox{ aus } \vec{r}(t), \vec{\dot{r}}(t) \\
\mbox{ 2. Schritt: } &  F( \vec{r}_1 (t+h),..., \vec{r}_N (t+h)) \\
\mbox{ 3. Schritt: } & \vec{\dot{r}}(t+h) \mbox{ aus } \vec{r}(t), \vec{r}(t+h), \vec{\dot{r}}(t)
\end{align}
wichtiges Kriterium für Güte eines Algorithmus:\\
\underline{Erhaltung der Energie:} hier: 
\begin{itemize}
\item Mittelwert stabil
\item momentane Werte schwanken
\end{itemize}
\underline{Kräfte:} zuer Beschreibund eines Festkörpers: QM-Rechnung der Kräfte auf Atome (sog. ab-initiv Rechnungen) \\
$\Rightarrow$ Energie als Funktion der Atomposition \\
$\Rightarrow$ Gitterstruktur, Gleichgewichtsabstände, Kräfte \\
für Gase, Flüssigkeiten gibt es das stark vereinfacht:
\\
\underline{Lennard-Jones-Potential} 
\begin{align}
U(r) = 
4 \epsilon 
\left[ \left(  \frac{ \sigma }{r}  \right)^{12} - \left( \frac{\sigma }{r} \right)^6 \right]
\end{align}
für neutrale Atome ohne innere Freiheitsgrade. 
\begin{itemize}
\item kleine Abstände: Wellenfunktion zweier Atome überlappen, Pauli Prinzip verbreitet Annäherung (bei abgeschlossenen Schalen im gleichen Zustand) $\Rightarrow U \propto r^{-12}$ (gut für Edelgase) \\
\item große Abstände: Van der Waals Kräfte durch Polarisation der Ladungsverteilung mit $U \propto - r^{-6}$.
\item eventuell auch weitere Wechselwirkungen wie z.B. Coulomb...
\end{itemize}
Beispiel: N Teilchen mit Lennard-Jones Potential
\begin{itemize}
\item Anfangsbedingung: $\vec{v}_i (0)=0, \vec{r}_i (0)$ äquidistant, Eindimensional (1D) 
\end{itemize}
zu 2) ereignisgesteuerte Simulation
$\vec{r}_i^{(0)}, \vec{v}_i^{(0)}$ seinen Koordinaten nach einem Stoß (oder Anfangsbedingung).
\begin{itemize}
\item Bilde alle $\frac{N(N-1)}{2}$ Abstände zwischen dem Teilchen $ \vert \vec{r}_i (t) - \vec{r}_j (t) \vert > R_i + R_j \; \forall i,j \Rightarrow$ freier Flug (freier Flug: $\vec{r}_i (t) = \vec{r}^{(0)} + \vec{v}^{(0)} t$)
\item Stoß, wenn für ein Paar $(i,j) \; \vert \vec{r}_i (t_s) - \vec{r}_j (t_s) \vert = R_i + R_j $ und $ \vert \vec{r}_i (t_s) - \vec{r}_j (t_s) \vert > R_i + R_j \; \forall 0< t < t_s $ und alle anderen $i,j$.
\end{itemize}
\begin{enumerate}
\item Berechnung der Stoßzeit (für gleiche Radien): 
\begin{align}
\vec{r}_i^{(0)} - \vec{r}_j^{(0)} \vert > R_i + R_j + \left( \vec{v}_i^{(0)} \right) -\left( \vec{v}_j^{(0)} \right) t_{s,ij}  \vert = 2 R \\
\Rightarrow (\Delta \vec{r}_{i,j})^2 + 2\Delta \vec{r}_{i,j} \Delta\vec{v}_{i,j} t_{s,ij} + (\Delta \vec{v}_{i,j})^2 t_{s,ij}^2 = 4R \\
t_{s,ij} =
 \frac{1}{(\Delta \vec{v}_{i,j})^2} 
 \left( 
 - \Delta \vec{r}_{i,j} \Delta \vec{r}_{i,j} - \sqrt{( \Delta \vec{v}_{i,j} \Delta \vec{r}_{i,j})^2 - (\Delta \vec{r}_{i,j}) - 4R^2 (\Delta \vec{v}_{i,j})^2} \right)
\end{align}
Stoßzeit $t_s = min \{ t_{s,ij} > 0\} \forall i,j$
\item Geschwindigkeitsänderung bei Stoß
\begin{itemize}
\item Impulsänderung $|| \vec{r}_{i} - \vec{r}_{j}$
\item Änderung von $\vec{v}_{i}, \vec{v}_{j}$ folgt aus Energiesatz und Impulssatz (hier: gleiche Massen, harte Kugeln)\\
\underline{Impulssatz}: $m( \vec{v}_{i}' - -\vec{v}_{i}) = -m ( \vec{v}_{j}' - \vec{v}_{j}) $ mit $(v' \triangleq$ nach Stoß) \\
\underline{Energiesatz:}
\begin{align}
\frac{m}{2} (\vec{v}_{i}^2 + \vec{v}_{j}^2) = \frac{m}{2} (\vec{v}_{i}'^2  + \vec{v}_{j}'^2) \\
 \mbox{ mit }  \vec{v}_{i}^2 - \vec{v}_{i}'^2 = \vec{v}_{j}^2 + \vec{v}_{j}'^2 
 (\vec{v}_{i}^2-\vec{v}_{i}'^2)(\vec{v}_{i}^2 + \vec{v}_{i}'^2) = (\vec{v}_{j}^2 - \vec{v}_{j}'^2)(\vec{v}_{j}^2 + \vec{v}_{j}'^2) \\
 \mbox{ wegen Impulserhalt. } v_i + v_i' = v_j' + v_j 
\end{align}
und weiterhin
\begin{align}
\mbox{ Impuls: } v_i'-v_i= v_j -v_j' = v_j - (v_i + v_i - v_j) = 2 v_i - v_i -v_i' \\
\Rightarrow v_i' - v_i = v_j - v_i \\
\mbox{ allgemein: } 
\Aboxed{ \Delta \vec{v}_{i} = \vec{l}_{i,j} (\vec{r}_j - \vec{r}_i)  \vec{l}_{i,j}  }
\mbox{ (3D) mit } \vec{l}_{i,j} = \frac{\vec{r}_i - \vec{r}_j}{\vert \vec{r}_i - \vec{r}_j \vert}
\end{align}
\end{itemize}
\end{enumerate}
 Manchmal ist auch eine Energiedissipation erwünscht: \\

\begin{align}
\Rightarrow \Delta \vec{v}_i = \vec{l}_{i,j} \left[ \vec{v}_j - \vec{v}_i) - \frac{Q}{m \Delta v_i} \right] \vec{l}_{i,j}
\end{align}
typische Annahme: $Q \propto \Delta v_i^2$ 
\begin{align}
\Rightarrow \frac{Q}{m}= \epsilon \Delta v_i^2 \Rightarrow v_i ' = v_i + (v_j - v_i) - \epsilon\underbrace{ \Delta v_i}_\text{= $v_j -v_i$} = \epsilon v_i + (1-\epsilon) v_j
\end{align}
$\epsilon$ ist dabei ein Maß für die beim Stoß dissipative Energie.
\subsubsection*{Randbedingungen:}
In Simulationen nur endliche Teilchenzahl $N$ möglich. \\
Wünschenswert: $N \to \infty, V \to \infty$ (Außer bei Nanotechnologie) \\
Beispiel:  N Teilchen auf einfach kubischem Gitter $N = 512 = 8^3$ \\
$\Rightarrow$ Oberfläche: $8 \cdot 8 \cdot 6 = 384$ Teilchen auf der Oberfläche (mehr als die Hälfte). \\
$\Rightarrow$ Normal: $6$ Nachbarn, auf Oberfläche $<6$ Nachbarn. \\
$\Rightarrow$ Problem sind die Oberflächen wegen geändertem Verhalten. (wegen der Nachbarn)\\
Lösungsansatz: Periodische Randbedingungen. 
\begin{itemize}
\item Bewegung der Teilchen wird nur in \textit{einer} Zelle berechnet
\item Wechselwirkung wird (prinzipiell) als Summe der Wechselwirkungen mit allen Bildern genommen
\end{itemize}
\underline{Einfaches Beispiel:} 1-dim, granulare Materie (geht auch in 2,3D) \\
\underline{komplizierter:} langreichweitige Wechselwirkung
Wechselwirkung:
\begin{itemize}
\item entweder abschneiden
\item oder unendliche Reihe ausrechnen (Ewald Summation)
\end{itemize}
\underline{Ergebnis:} Oberflächeneffekte verschwinden, aber System bleibt periodisch. \\
Beispiel: Event driven molekulardynamische Simulation, ($N=10$,$1d$, $R=0$) (links getrieben, rechts reflektiert $v  \dashrightarrow -v$)

\subsection{Stochastische Differentialgleichungen}
\underline{Bislang:}
\begin{itemize}
\item klassische Bewegungsgleichungen lösen $\Rightarrow$ Moolekulardynamik \textit{ohne} Ankopplung an Wärmebad 
\item Ankopplung an Wärmebad (Mastergleichung) ohne Bewegungsgleichung
\end{itemize}
\underline{Jetzt:} Versuchen Bewegungsgleichung zu erweitern, sodass Wärmebad beschrieben wird
 \begin{itemize}
 \item Wandstöße sorgen für zufällige Impulsübertragung \\
 $\Rightarrow$ Bewegungsgleichung muss um einen \textit{zufälligen} Beitrag erweitert werden:
 \begin{align}
 \dot{x} =\underbrace{ F(x(t))}_\text{deterministisch} +\underbrace{ \eta (t)}_\text{stochastisch} \\
 dx = f(x(t))dt + d\omega(t)
 \end{align}
  \end{itemize}
\underline{Beispiel:} \textsc{Brown}'sche Molekularbewegung (\textsc{Brown} 1827, \textsc{Einstein} 1905, \textsc{Smoluchowski} 1906 $\to$ \textsc{Fokker-Planck}-Gleichung, \textsc{Langevin} 1908) 
Folgen \textsc{Langevin} Bewegungsgleichung für ein Teilchen auf einer Flüssigkeit. \textsc{Langevin} Gleichung 
\begin{align}
\underbrace{\dot{v}}_\textsc{Newton}= \underbrace{-\frac{c}{m} v(t)}_\text{ \textsc{Stokes}'sche Reibung} + \underbrace{\frac{1}{m}\eta (t)}_\text{Rauschen} \; , \; \mbox{ mit } c= 6 \pi \nu a
\end{align} (Bewegungsgleichung für ein Teilchen, da ja in Wirklichkeit Wechselwirkung mit anderen Teilchen..ohne Wechselwirkung hätte man gleich ganz viele Teilchen nehmen können)

\underline{Problem:} Deterministischer Anteil gibt eine Lösung für eine Anfangsbedingung. Durch Rauschen wird Trajektorie 'zufällig'. $\Rightarrow$ müssen ein Ensemble von Trajektorien betrachten.\\
Eigenschaften des Rauschens $\eta (t)$ 
\begin{enumerate}
\item Mittelwert $\langle \eta \rangle = 0 $
\begin{align}
\int_t^{t + \Delta t} \eta (t') dt' \; \to \mbox{ \textsc{Gauss}-Verteilung. (Summe von Zufallsprozessen in } (t, t+\Delta t) )
\end{align}

\item Rauschen ist unkorreliert 
\begin{align}
 \langle \eta(t_1), \eta (t_2) \rangle =& \Gamma \delta (t_1 - t_2)\\
 \langle\Delta \omega(t_2), \Delta \omega(t_2)\rangle
 =& \langle \int_{t_1}^{t_1 + \Delta t} \eta (t') dt'  \int_{t_2}^{t_2 + \Delta t} \eta (t'') dt'' \rangle 
 = \Gamma  \int_{t_2}^{t_2 + \Delta t} \int_{t_1}^{t_1 + \Delta t} dt' dt'' \; \delta(t'-t'') \\
 =& \Gamma \Delta t\;  \delta_{t_1,t_2}  \\
 \langle \Delta \omega \rangle =& \langle \int_t^{t+\Delta t} dt' \eta (t') \rangle =0
\end{align}
\end{enumerate}
Deterministischer Anteil hat Lösung
\begin{align}
v(t) = v_0 e^{- \frac{t}{\tau}} \; , \; \tau = \frac{m}{c}
\end{align}
mit Rauschen: Berechnen Autokorrelationsfunktion:
\begin{align}
v(t+ \Delta t ) - v(t) =& - \frac{c}{m} v(t) \Delta t + \frac{1}{m} \int_t^{t+\Delta t} \eta (t') dt' \\
\underbrace{ \langle v(0)}_\text{einfach eine Konstante} v(t+ \Delta t ) \rangle - \langle v(0) v(t) \rangle =&- \frac{c}{m} \langle v(0) v(t) \rangle \Delta t + \frac{1}{m} \underbrace{ \langle v(0) \int_t^{t+\Delta t} \eta (t') dt' \rangle }_\text{$=0$} \\
\Rightarrow \frac{d}{dt} \langle v(0) v(t) \rangle 
=& \frac{\langle v(0) v(t+ \Delta t) \rangle - \langle v(0) v(t) \rangle}{\Delta t} = - \frac{c}{m} \langle v(0) v(t) \rangle \\
\langle v(0) v(t) \rangle =& \langle v(0)^2 \rangle e^{-\frac{t}{\tau}}
\end{align}
Aus statistischer Mechanik
\begin{align}
\langle \frac{m}{2} v^2 \rangle = \frac{k_B T}{2} \\
\Rightarrow \mbox{ Autokorrelationsfunktion: } 
\langle v(t) v(t') \rangle = \frac{k_B T}{m} e^{- \frac{t-t'}{\tau}}
\end{align}
Betrachten $\tau \to 0$ (entspricht $ m\to 0$), dann
\begin{align}
\langle v(t) v(t') \rangle \approx & \frac{2 k_B T}{m} \tau \delta (t_t') =\frac{2 k_B T}{c} \delta (t_t') \\
v=& \frac{dx}{dt}= \frac{1}{c} \eta (t) \; \mbox{ mit } \langle \eta (t') \eta (t) \rangle = \Gamma \delta (t-t') 
\end{align}
\begin{enumerate} 
\item
\begin{align}
\Gamma =& c^2 \frac{2 k_B T}{c} = 2 k_B T c
\end{align}
\item lösen \begin{align}
\dot{x}= & \frac{1}{c} \eta (t) \; \Rightarrow x(t) = x(0) + \int_0^t \frac{\eta (t')}{c} dt' \\
\langle x(t) \rangle =& \langle x(0) \rangle = 0 \\
\langle (x(t) - x(0))^2 \rangle =& \frac{1}{c^2} \int_0^t \int_0^t dt' dt'' \underbrace{\langle \eta (t') \eta (t'') \rangle}_\text{$ \Gamma \delta (t' - t'')$} = \frac{2 k_B T}{c^2} c t = \underbrace{2 D t = \frac{2 k_B T t}{c}}_\text{\textsc{Einstein}-Beziehung}
\end{align}
$D$ ist die Diffusionskonstante mit \begin{align}
D= \frac{k_B T}{6 \pi \nu a} \; \mbox{ mit: Viskosität } \nu \mbox{ und Radius } a
\end{align}
Ausbreitung von \textit{Random Walk} $\propto \sqrt{t}$ $\Rightarrow$ Wiener Prozess
\end{enumerate}
\underline{Random Walk:} eines der einfachsten Modelle der Computersimulation
\begin{itemize}
\item 'Random Walker' startet auf Gitterpunkt
\item bei jedem Zeitschritt: zufällige Bewegung auf Gitter
\item einzelne Trajektorie zufällig $\Rightarrow$ statistische Auswertung durch Mittelung über viele RW's
\begin{align*}
\langle x(t) \rangle =& \langle x(0) \rangle = x(0) \\
\langle x(t)^2 \rangle = & t \; \Rightarrow \sqrt{\langle x(t)^2 \rangle} \propto \sqrt{t}
\end{align*}
Ausbreitung der RW also $\propto \sqrt{t}$ in jeder Dimension $\Rightarrow$ für Diffusion
\end{itemize}
\subsection*{Numerik stochastischer Differentialgleichungen (SDGL)}
\begin{enumerate}
\item Additives Rauschen der Form 
\begin{align}
\dot{x}	= f(x) + \eta (t)
\end{align}
$\eta (t)$: stochastischer Prozess und
\begin{align}
\langle \eta (t) \rangle = 0 \; , \; \langle \eta (t), \eta (t') \rangle = \Gamma \delta (t-t') \forall t
\end{align}
Diskretisieren der Zeit $t_n = n \Delta t \forall n \in \mathcal{N}$
\begin{itemize}
\item ohne Rauschen 
\begin{align}
\underbrace{\int_t^{t+\Delta t} \dot{x} dt}_\text{$x(t+ \Delta t) - x(t)$} = \underbrace{\int_t ^{t+\Delta t} f(x(t')) dt'}_\text{$f(x(t)) \Delta t + \mathcal{O}(\Delta t^2)$} \\
x_{n+1} = x_n + f(x_n) \Delta t + \mathcal{O} (\Delta t^2) \mbox{ 'Euler Verfahren'}
\end{align}
lokaler Abbruchfehler ist von Ordnung $\mathcal{O} (\Delta t^2)$
\item mit Rauschen 
\begin{align}
x_{n+1} = x_n + f(x_n) \Delta t + \Delta w(t_n) \mbox{ wobei } \Delta W(t_n) = \int_{t_n}^{t_n + \Delta t} \eta (t) dt \\
\langle \Delta w(t_n) \Delta w(t_{n'}) \rangle = \langle \int_{t_n}^{t_n +\Delta t } \int_{t_{n'}}^{t_{n'} +\Delta t } \eta (t_1) \eta (t_2) dt_1 dt_2 \rangle \\
= \int_{t_n}^{t_n +\Delta t } \int_{t_{n'}}^{t_{n'} +\Delta t }  \Gamma \delta (t_1 - t_2) dt_1 dt_2 = \Gamma \int_{t_n}^{t_n +\Delta t } \delta_{n n'} dt_1 = \Gamma \Delta t \delta_{n n'}
\end{align}
$\to$ weißes Rauschen mit $\Gamma \Delta t$, siehe (a)\\
$\to$ stochastische Eigenschaften bleiben erhalten aber $\Delta x$ wird erhöht um $ \Delta x \propto \sqrt{\Delta t}$ in niedrigster Ordnung (statt $\Delta t$)
\begin{align}
\Rightarrow f(x_n + \Delta x) \approx f(x_n) + \mathcal{O} (\Delta x) = f(x_n) + \mathcal{O}(\Delta t^\frac{1}{2}) \\
\Rightarrow  \Aboxed{
 x_{n+1} = x_n + f(x_n) \Delta t + \Delta w (t_n) + \mathcal{O} (\Delta t^\frac{3}{2})}
\end{align}
$\to$ mit Rauschen: andere Konvergenz \\
$\to$ andere Breite des Rauschens für endliche Zeitschritte $(\Gamma \to \Gamma \Delta t)$
\item Praktische Durchführung des \textsc{Euler-Maruyama}-Verfahrens
\begin{align}
\dot{x}= \underbrace{ f(t,x)}_\text{det. Teil} +\underbrace{ g(t) \underbrace{\eta (t)}_\text{Rauschen}}_\text{stoch. Teil}
\end{align}
zusammen mit
\begin{align}
\langle \eta (t) \rangle = 0 \; , \; \langle \eta (t) \eta (t') \rangle = \Gamma \delta (t-t')  \label{conditions}\\
\Aboxed{ x_{n+1}= x_n + f(t_n, x_n) \Delta t + g (t_n) \Delta w}\\ 
\mbox{ \textsc{'Euler-Maruyama}-Verfahren'}
\end{align}
wobei 
\begin{align}
P( \Delta w) = \frac{1}{\sqrt{2 \pi} \mathcal{O}} e^{-\frac{\Delta w^2}{	2 \mathcal{O}^2}} \; , \; \mathcal{O}^2 = \Gamma \Delta t \\
\langle \Delta w \rangle = 0 \; , \; \langle \Delta w^2 \rangle = \Gamma \Delta t
\end{align}
Beachte: Gaußverteilung ist nicht notwendig um (\ref{conditions}) zu erfüllen, aber hinreichend.
\end{itemize}
Beispiel: \textsc{Brown}'sche Molekularbewegung 
\begin{align*}
m \ddot{x}= - \gamma \dot{x} + \eta (t) \\
\Rightarrow z= 
\begin{pmatrix}x\\ \dot{x}\end{pmatrix} 
\Rightarrow  \dot{z} = \begin{pmatrix} \dot{x}\\ \ddot{x}\end{pmatrix} = \begin{pmatrix}z_1\\ - \frac{\gamma}{m} z_2 + \frac{1}{m} \eta (t)\end{pmatrix} = 
\underbrace{\begin{pmatrix} Z_2 \\ \frac{-\gamma}{m} z_2 \end{pmatrix} }_\text{det Teil}+\underbrace{ \begin{pmatrix} 0\\ \frac{1}{m} \eta (t)\end{pmatrix} }_\text{stoch}
\end{align*}
Beachte: \textit{kein} Wiener Prozess , da endliche Masse die zur Beschleunigung für kurze Zeiten führt. $\to$ Abweichung von $\langle ( x(t) - x(0)^2) \rangle = 2Dt$
\item Multiplikatives Rauschen der Form 
\begin{align}
\dot{x}= f(t,x) + g(t,x) \cdot \eta (t) 
\end{align}
\begin{enumerate}
\item Möglichkeit: Diskretisierung wie \textsc{Euler}
\begin{align}
\Delta x = x(t + \Delta t) - x(t) = f(x) \Delta t + g(x) \Delta w + \mathcal{O}(\Delta t^\frac{3}{2} \label{formel}
\end{align}
\item Möglichkeit: \textsc{Heun}-Verfahren (\textit{mit Parameter $\alpha$})
\begin{align}
\Delta x = \frac{\Delta t}{2} \left( f(x) + f(x + \alpha \Delta x) \right)+  \frac{\Delta w}{2} \left( g(x) + g(x +\alpha \Delta x) \right)
\end{align}
\begin{align}
f(x + \alpha \Delta x ) =& f(x) + f'(x) \alpha \Delta x + \mathcal{O}(\Delta x^2) \\
g(x + \alpha \Delta x ) =& g(x) + g'(x) \alpha \Delta x + \frac{1}{2} g'' (x) (\alpha \Delta x)^2+ \mathcal{O}(\Delta x^3) \\
\Rightarrow \Delta x = & f(x) \Delta t + \frac{1}{2} f' (x) \alpha \Delta x \Delta t + \mathcal{O} (\Delta x^2 \Delta t) \\
+&
g(x) \Delta g + \frac{1}{2} g' (x) \alpha \Delta x \Delta w +\frac{\alpha^2}{4} g''(x) \underbrace{\Delta x^2 \Delta w}_\text{$ = \mathcal{O}(\Delta t^\frac{3}{2}$} + \underbrace{\mathcal{O} (x^3) }_\text{$ \Delta t^\frac{3}{2}$}
\\
 \mbox{ mit: } \mathcal{O}(\Delta x) = \mathcal{O}(\Delta t^{\frac{1}{2}}
\end{align}
\begin{align}
\Rightarrow f(x) \Delta t + g(x) \Delta w + \frac{\alpha}{2} g'(x) g(x) \underbrace{\Delta w^2}_\textbf{$\mathcal{O}(\Delta t)$} + \mathcal{O} (\Delta t^\frac{3}{2} ) \label{formel2}
\end{align}
\begin{itemize}
\item (\ref{formel}) und (\ref{formel2}) unterscheiden sich um einen Term der Ordnung $\Delta t$, global ist die Abweichung dann von Ordnung $\mathcal{O}(1)$.
\item Spezialfall $\frac{\partial g}{\partial x}=0$ (additives Rauschen) $\Rightarrow$ (\ref{formel}) und (\ref{formel2}) stimmen überein
\item $\alpha =0$ ergibt \textsc{Euler}, $\alpha=1$ ergibt \textsc{Heun}
\item (\ref{formel}) approximierend SDGL nach Jtö
\item (\ref{formel2}) approximierend SDGL nach Stratonovisch
\end{itemize}
\end{enumerate}
\end{enumerate}
%TODO interruption because I was too late

\underline{Numerik:} 
$\alpha = 0 $ (\textsc{Euler}
) und $\alpha = 1$ (\textsc{Heun}) unterscheiden sich um Term der Ordnung $\Delta t \Rightarrow $ verschiedene Ergebnisse. \\ 
Die Verfahren konvergieren zu verschiedenen Integralen.

\begin{align} \Rightarrow \text{Ito-Stratonovich-Dilemma}
\begin{cases}
\textsc{Euler} \to \text{ITO} \\
\textsc{Heun} \to \textsc{Stratonovich}
\end{cases}
\end{align}

\underline{Beispiel:}
 \begin{align}
\dot{x}= \eta (t) \Rightarrow \langle x^2 \rangle = \langle x_0^2 \rangle + 2Dt
\end{align}
Transformation $y=x^2$:
\begin{align}
\Rightarrow \dot{y}= 2 x \dot{x} = 2 x \eta (t) = 2 \sqrt{y} \eta (t)
\end{align}
%TODO missing line!!

Betrachte Diskretisierung mit $f=0$ und $g= 2 \sqrt{y}$ 
\begin{align}
\Aboxed{\Delta y} = 2 \sqrt{y} \Delta w + \frac{\alpha}{2} 2 \frac{1}{2 \sqrt{y}} 2 \sqrt{y} \Delta w^2 = \Aboxed{2 \sqrt{y} \Delta w + \alpha \Delta w^2 }\\
\Rightarrow \langle \Delta y \rangle = \langle \alpha \Delta w^2 \rangle = \alpha 2 D \Delta t
\end{align}
Stimmt mit Lösung oben überein für $ \alpha = 1$ $\Rightarrow$ ' \textsc{Stratonovich} ist richtig'

\subsection{Zeitquantifizierung von Monte Carlo} %TODO stimmt die Nummerierung?
 Wir betrachten wie zuvor in Kapitel  \ref{Kapitel2.9}:
 Hamiltonian: \begin{align}
 H =\underbrace{ -\frac{J}{2} \sum_{i,j} \vec{S}_i \vec{S}_j }_\text{Austausch}- \underbrace{ D \sum_{i=1}^N (\vec{S}_i^2)^2 }_\text{Anisotropie}- \underbrace{\vec{B} \sum_{i=1}^N \vec{S}_i }_\text{\textsc{Zeeman}}
 \end{align}
 im klassischen Limes $\vert \vec{S} \vert $ 
 \begin{itemize}
 \item MC hatten wir schon
 \item jetzt \textsc{Langevin} Dynamik (Analogon zum Kreisel: Zwar nicht rotation aber präzession)
 
  Bewegungsgleichung: aus \textsc{Heisenberg}scher Bewegungsgleichung
  \begin{align}
  i \hbar \frac{\partial}{\partial t} \langle \vec{S}^{tilde}(t) \rangle = \langle \left[ \vec{S}(t) ,H^{tilde}\right] \rangle \\
  \Rightarrow \frac{d \vec{S}_i}{dt} = \frac{\gamma}{\mu } ( \vec{S}_i \times \vec{h}_i ) \mbox{ mit } \vec{h}_i = - \nabla_{\vec{S}_i} H \mbox{ für } h \to 0 \\
  \end{align}
  und weitere Annahmen (ist eigenltich eine Schwingung! Wie bei Phononen) (Der Vorfaktor hat was mit h-quer zu tun und kommt aus der quantenmechanischen Rechnung Raus: ein Spin S (Drehimpuls) hat magnetisches Moment $\mu$ und $\frac{g \mu_B}{\hbar}= \gamma$ wobei $g=$\textsc{Landé}Faktor (für 2 Elektronen), $\mu_B$: \textsc{Bohr}sches Magneton, $\gamma$: gyromagnetisches Verhalten. ) \\
  
  \underline{$+$Dissipation:} (nach Landau-Lifshitz)
  \begin{align}
  \dot{\vec{S}_i}= -\frac{\gamma}{\mu} (\vec{S}_i \times \vec{h}_i ) - \frac{\alpha \gamma}{\mu} (\vec{S}_i \times ( \vec{S}_i \times \vec{h}_i))
  \end{align}
  $\alpha$: Dämpfungskonstante \\
  \underline{$+$ Fluktuation:} addiere weißes Rauschen zum effektiven Feld $\vec{h}_i: \vec{h}_i \to \vec{h}_i + \vec{\eta} (t)$
  mit $\langle \vec{\eta}_i (t) \rangle = 0$ und
   $\langle  \vec{\eta}_i^\nu (t)  \;  \vec{\eta}_j ^\theta (t) \rangle = \frac{2 \alpha \mu k_B T}{\gamma} \underbrace{\delta_{ij}}_\text{Gitterplätze} \underbrace{\delta_{\nu, \theta}}_\text{kart. Komponenten} \delta (t- t')$
   \item führt zu Gleichgewichtseigenschaften des kanonischen Ensembles
   \end{itemize}
   \underline{Einfaches Beispiel:} 1 Makrospin mit 
   \begin{align*}
   H = -D S_z^2 - \vec{B} \vec{S}
   \end{align*}
   Modell für Superparamagnetsimus: Für $\Delta E = D \approx k_B T$ kann die Energiebarriere thermisch überwunden werden. \\
   \underline{Theorie:} mittlere Zeit $\tau$ zum überspringen der Energiebarriere ist $\tau = \tau_0 e^\frac{\Delta E}{k_B T}$. \begin{itemize}
   \item Für $B=0$ ist $\Delta E = D$ und $\tau $ bekannt $\tau (D, T)$. 
    \item Ansonsten lässt sich $\tau, \Delta E$ asymptotisch rechnen.
    \end{itemize}
    \underline{Numerik:} \textsc{Langevin:} starten in Energieminimum. Wie wachsen Fluktuationen?
    \begin{align}
    \dot{\vec{S}} = - \frac{\gamma}{\mu} \vec{S} \times \left( ( \vec{h} +  \vec{\eta}) + \alpha  \vec{S} \times ( \vec{h} +  \vec{\eta}) \right)
    \end{align}
    Linearisieren:
    \begin{align}
     \vec{S} = \begin{pmatrix}S_x\\ S_y \\ 1 \end{pmatrix}
    \end{align}
    weil 
    \begin{align}
    S_z = \sqrt{1- S_x^2 - S_y^2}
    \end{align}
    \begin{align}
    \Rightarrow \dot{\vec{S}}_x = - \frac{\gamma }{\mu} \left(
    \underbrace{ S_y}_\text{ $S_y <<1$} ( h_z - \eta_z ) - \underbrace{S_z}_\text{$=1$} (h_y + \eta_y) \right) - \underbrace{\alpha....}_\text{kleiner für $ \alpha \to 0$}
    \end{align}
    mit
    \begin{align}
    \vec{B} = \begin{pmatrix} 0 \\ 0 \\ B_z \end{pmatrix} \Rightarrow h_y = 0 \mbox{ und } \vec{h}= - \frac{\partial H}{\partial \vec{S}}= (2 DS_z + B_z) \hat{\vec{z}}
    \end{align}
    \begin{align}
    \Rightarrow \dot{\vec{S}}_x \approx & \frac{\gamma}{\mu} \eta_y (t) \Rightarrow \langle \Delta S_x^2 \rangle = Dt = \frac{\gamma ^2}{\mu ^2} \frac{2 \alpha \mu }{\gamma} k_B T \Delta t \\
   \Aboxed{  \dot{\vec{S}}_z = & 2 \frac{\gamma}{\mu} \alpha k_B T \Delta t = \langle \Delta S_y^2 \rangle}
    \end{align}
    Wir berechnen $\langle \Delta S_x^2 \rangle $ für einen \textsc{Monte Carlo} Schritt: Dabei ist $r = \sqrt{S_x^2 + S_y^2}$ die Schrittweite und $R$ die Maximale Schrittweite.
    \begin{itemize}
    \item Wahrscheinlichkeitsverteilung für Versuchsschritt $p_t (r)$
    \item Wahrscheinlichkeitsverteilung für Akzeptanz $p_a (r) = \frac{1}{1 + e^\frac{\Delta E(r)}{k_B T}}$ (\textit{heat bath}).
    \end{itemize}
    \begin{align}
p_t (r) = & \frac{3 \sqrt{ R^2 - r^2}} { 2 \pi R^3}     \; \Rightarrow \langle\Delta S_x^2 \rangle = \langle \frac{r^2}{2} \rangle = \int_0^{2 \pi} d \phi \int_0 ^R r dr \frac{r^2}{2} p_r (r) p_a (r)   \\
=& 2 \pi  \int_0^R dr \frac{r^3}{2} \frac{3 \sqrt{R^2 - r^2}}{2\pi R^3 \left( 1 + \underbrace{ e^\frac{\Delta E (r) }{k_B T}}_\text{$ 1+ \frac{\Delta E (r)}{k_B T}$} \right)} 
\\ \approx & \frac{3}{4 R^3} \int_0^R dr r^3 \sqrt{R^2 - r^2} \approx \frac{R^2}{10}
\end{align}
Vergleich:  \\
\begin{center}


\begin{tabular}{ c c }
\textsc{Langevin:} & \textsc{Monte Carlo} \\ 
$\langle \Delta S_x^2 \rangle = 2\frac{\gamma}{\mu} \alpha k_B T \Delta t $ & $ \langle \Delta S_x^2 \rangle = \frac{R^2}{10} $\\ 
\end{tabular} \\
\end{center}

$\Rightarrow $ 1 MCS entspricht Zeitintervall $\Delta t$ für
\begin{align}
\Aboxed{ R^2 = \frac{20 k_B T \alpha \gamma}{\mu} \Delta t }
\end{align}
für $R<<1$
 \end{document}
 Siehe physical review letters 3. January 2000 Volume 84 Number 1
 
 
 
 
 
 
 
 
 ######################
 komma oder punkt als dezimaltrennung?